{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84VetyCaGLyR"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you will implement some of the techniques discussed in the lecture.\n",
        "\n",
        "Below you are given a solution to the previous scenario. It has two serious drawbacks:\n",
        " * The output predictions do not sum up to one (i.e. the output is not a probability distribution), even though the images always contain exactly one digit.\n",
        " * It uses MSE coupled with output sigmoid, which can lead to saturation and slow convergence.\n",
        "\n",
        "**Task 0.** Implement a numerically stable version of softmax.\n",
        "\n",
        "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block, rather than compute the gradient over the softmax values.\n",
        "\n",
        "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
        "\n",
        "**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n",
        "\n",
        "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n",
        "\n",
        "The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iZypYewcXywA",
        "outputId": "4ec06fcb-8a66-40cc-d009-0c83cd6df742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "2025-11-18 14:09:30 URL:https://s3.amazonaws.com/img-datasets/mnist.npz [11490434/11490434] -> \"mnist.npz\" [1]\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm pandas\n",
        "!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "P22HqX9AbO1a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from numpy.typing import NDArray\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "FloatNDArray = NDArray[np.float64]\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "N9jGPaZhbO2B"
      },
      "outputs": [],
      "source": [
        "def load_mnist(\n",
        "    path: Path = Path(\"mnist.npz\")\n",
        ") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n",
        "\n",
        "    Returns tuple of:\n",
        "    - x_train: shape (N_train, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n",
        "    - x_test: shape (N_test, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n",
        "\n",
        "    More: https://en.wikipedia.org/wiki/MNIST_database\n",
        "    \"\"\"\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n",
        "        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n",
        "\n",
        "    H = W = 28\n",
        "    N_train = len(x_train)\n",
        "    N_test = len(x_test)\n",
        "    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n",
        "    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n",
        "\n",
        "    x_train = x_train.reshape(N_train, H * W) / 255.0\n",
        "    x_test = x_test.reshape(N_test, H * W) / 255.0\n",
        "\n",
        "    y_train = np.zeros((N_train, 10), dtype=np.float64)\n",
        "    y_train[np.arange(N_train), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((N_test, 10))\n",
        "    y_test[np.arange(N_test), _y_test] = 1\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "w3gAyqw4bO1p"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: FloatNDArray) -> FloatNDArray:\n",
        "    #print(\"HAHA\")\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVG90fCXywB"
      },
      "source": [
        "## Warm-Up\n",
        "Implement a numerically stable version of softmax.  \n",
        "\n",
        "In general, softmax is defined as  \n",
        "$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n",
        "However, taking $e^{1000000}$ can result in NaN.  \n",
        "Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n",
        "\n",
        "Hint: <sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "_rutQhoaXywC",
        "outputId": "93a13214-1f5a-41e0-fd23-0327b104a2ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "def unstable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def stable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    ## TODO\n",
        "    maxX = np.max(x)\n",
        "    xNorm = x - maxX\n",
        "\n",
        "    e = np.exp(xNorm)\n",
        "\n",
        "    sum = np.sum(e,axis=axis, keepdims=True) + 1e-10  # this makes the sum 100 x 1, and not 100, needed for\n",
        "    result = (e / sum)\n",
        "    return result\n",
        "\n",
        "\n",
        "### TESTS ###\n",
        "def _test_one(x: FloatNDArray, y: FloatNDArray) -> None:\n",
        "    r = stable_softmax(x)\n",
        "    assert r.shape == y.shape, f\"Expected shape {y.shape}, got {r.shape=}\"\n",
        "    assert np.isclose(np.ones(x.shape[0]), r.sum(axis=-1), atol=1e-5, rtol=0).all()\n",
        "    assert np.isclose(y, r, atol=1e-5, rtol=0).all()\n",
        "\n",
        "def test_stable_softmax() -> None:\n",
        "    x1 = np.random.rand(100, 32).astype(np.float64)\n",
        "    _test_one(x1, unstable_softmax(x1))\n",
        "\n",
        "    x2 = np.ones((10, 10, 32), dtype=np.float64) * 1e6\n",
        "    _test_one(x2, np.ones_like(x2) / x2.shape[-1])\n",
        "\n",
        "    print(\"OK\")\n",
        "\n",
        "test_stable_softmax()\n",
        "### TESTS END ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModelResults\n"
      ],
      "metadata": {
        "id": "WWFnNWF8fWJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "FgEA2XRRbO2X"
      },
      "outputs": [],
      "source": [
        "class ModelResults:\n",
        "    \"\"\"Just a helper class for gathering results in a nice table. Feel free to ignore.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Map from model name to map from lr to list of test accuracies.\n",
        "        self.results = dict[str, dict[float, list[float]]]()\n",
        "\n",
        "    def clear(self, model_name: str | None = None) -> None:\n",
        "        \"\"\"Forget results for a given model (defaults to all models).\"\"\"\n",
        "        if model_name:\n",
        "            if model_name in self.results:\n",
        "                del self.results[model_name]\n",
        "        else:\n",
        "            self.results = {}\n",
        "\n",
        "    def add_result(self, model_name: str, learning_rate: float, accuracy: float) -> None:\n",
        "        if model_name not in self.results:\n",
        "            self.results[model_name] = {}\n",
        "        if learning_rate not in self.results[model_name]:\n",
        "            self.results[model_name][learning_rate] = []\n",
        "        self.results[model_name][learning_rate].append(accuracy)\n",
        "\n",
        "    def display_results(self) -> None:\n",
        "        data = list[dict[str, Any]]()\n",
        "        for model_name, model_results in self.results.items():\n",
        "            for lr, accuracies in model_results.items():\n",
        "                mean_accuracy = np.mean(accuracies)\n",
        "                accuracy_summary = f\"{mean_accuracy:2.1%} ± {np.std(accuracies) * 100:.1f} p.p.\"\n",
        "                data.append({\n",
        "                    \"model\": model_name,\n",
        "                    \"lr\": lr,\n",
        "                    \"mean_accuracy\": mean_accuracy,\n",
        "                    \"accuracy\": accuracy_summary\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(data).sort_values(\"mean_accuracy\", ascending=False)\n",
        "        del df[\"mean_accuracy\"]\n",
        "        display(df.style.format({\"lr\": \"{:.1g}\"}).hide())\n",
        "\n",
        "    def evaluate_model(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        model_constructor: Callable[[Sequence[int]], Any],\n",
        "        layers: Sequence[int] = (784, 30, 10),\n",
        "        learning_rates: Sequence[float] = (1.0, 10.0, 100.0),\n",
        "        n_trainings: int = 3,\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        # Automatic model name with parameters.\n",
        "        if kwargs:\n",
        "            if tuple(layers) != (784, 30, 10):\n",
        "                model_name += \"[\" + \",\".join(str(n) for n in layers) + \"]\"\n",
        "\n",
        "            model_name += \"(\"\n",
        "            for k, v in kwargs.items():\n",
        "                if isinstance(v, (float,  np.floating)):\n",
        "                    model_name += f\"{k}={v:.1g},\"\n",
        "                else:\n",
        "                    model_name += f\"{k}={v},\"\n",
        "            model_name = model_name[:-1]\n",
        "            model_name += \")\"\n",
        "\n",
        "        # Train for each learning rate, n_trainings times.\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Checking {n_trainings} random trainings with with lr = {lr}\")\n",
        "            for i in range(n_trainings):\n",
        "                network = model_constructor(layers, **kwargs)\n",
        "                accuracy = network.train(\n",
        "                    (x_train, y_train),\n",
        "                    epochs=10,\n",
        "                    mini_batch_size=1,\n",
        "                    learning_rate=lr,\n",
        "                    test_data=(x_test, y_test),\n",
        "                )\n",
        "                self.add_result(model_name, lr, float(accuracy))\n",
        "\n",
        "\n",
        "model_results = ModelResults()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "The solution to the previous lab: an MLP network with MSE loss on sigmoid outputs, trained with plain SGD (batched)."
      ],
      "metadata": {
        "id": "kJD-EMvq6H2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: H * W = 28 * 28 = 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        \"\"\"\n",
        "        self.sizes = list(sizes)\n",
        "\n",
        "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n",
        "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
        "\n",
        "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
        "        # Weights are indexed by target node first.\n",
        "        self.weights = [\n",
        "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
        "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
        "        ]\n",
        "\n",
        "        self.num_layers = len(self.weights)   # = len(sizes) - 1\n",
        "\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n",
        "\n",
        "        Returns last layer activations, shape (B, N^last), values 0..1.\n",
        "        \"\"\"\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i) + (N^i,)  ==  (B, N^i)\n",
        "            g = sigmoid(g @ w.T + b)\n",
        "        return g\n",
        "\n",
        "\n",
        "    def forwardStep(self, x: FloatNDArray): # forward step that also returns preactivations and activations of all neurons/layers\n",
        "      g = x\n",
        "      #print(\"FEED: \")\n",
        "      fs = []\n",
        "      gs = []\n",
        "\n",
        "      for w, b in zip(self.weights, self.biases, strict=True):\n",
        "\n",
        "        # TODO\n",
        "        #print(\"inputs shape: \", g.shape)\n",
        "        #print(\"Biases: \", b.shape, \" AND WEIGHTS: \", w.shape)\n",
        "        newF = np.matmul(w,g.transpose()).transpose() + b\n",
        "        #print(\"FS SHAPE: \", newF.shape)\n",
        "        fs.append(newF)\n",
        "\n",
        "        # print(\"NEW F: \", newF.shape, \"BIASES: \", b.shape)\n",
        "\n",
        "        newG = sigmoid(newF)\n",
        "\n",
        "        gs.append(newG)\n",
        "\n",
        "        g = newG\n",
        "      return fs, gs\n",
        "\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Gradient descent step.\n",
        "        self.weights = [\n",
        "            w - learning_rate * grad_w\n",
        "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - learning_rate * grad_b\n",
        "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        \"\"\"\n",
        "        Backpropagation for a mini-batch (vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input, shape (B, N^0)\n",
        "        - y: target label (one-hot encoded), shape (B, N^last)\n",
        "\n",
        "        Returns (grads_w, grads_b), where:\n",
        "        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
        "        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n",
        "        \"\"\"\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "        ### Copy from previous labs ###\n",
        "        # Pre-activations function, layer by layer, shapes (N^1), ..., (N^last).\n",
        "        fs: list[FloatNDArray] = []\n",
        "        # Activations (including inputs to the first layer), shapes (N^0), (N^1), ..., (N^last).\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "        pres, acts = self.forwardStep(x)\n",
        "        fs += pres\n",
        "        gs += acts\n",
        "\n",
        "        grad_g2 = self.cost_derivative(gs[-1], y) # shape initially (N^last), then layer by layer.\n",
        "\n",
        "        #print(\"SHAPE OF GRADG2: \", grad_g2.shape)\n",
        "\n",
        "        gradSigm2 = sigmoid_prime(fs[-1])\n",
        "\n",
        "        #print(\"SHAPE OF SIGM2: \", gradSigm2.shape)\n",
        "\n",
        "        localGrad2 = grad_g2 * sigmoid_prime(fs[-1]) # this should be 10x 1\n",
        "\n",
        "        #print(\"SHAEP OF LOCAL GRAD 2: \", localGrad2.shape)\n",
        "\n",
        "        past_g1 = gs[-2]\n",
        "        #print(\"SHAPE OF ACTS OF layer 1: \", past_g1.shape) # this is 100 x 30\n",
        "\n",
        "        # now compute the grads for each weight in each neuron\n",
        "\n",
        "        ##print(\"SHAPES OF LCOAL GRAD AND PAST G : \", grad_f.shape, past_g.shape)\n",
        "\n",
        "        grads_w_layer2 = np.matmul(localGrad2.transpose(), past_g1) # this should be 10 x 30\n",
        "\n",
        "        #print(\"SHAPE OF GRADS OF WEIGHTS OF LAYER 2: \", grads_w_layer2.shape) # up here correct\n",
        "\n",
        "        grads_b_layer2 = np.sum(localGrad2.transpose(), axis=-1)\n",
        "        # TODO backward pass.\n",
        "\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "\n",
        "        grads_w.append(grads_w_layer2)\n",
        "        grads_b.append(grads_b_layer2)\n",
        "\n",
        "        # now compute the other layers gradients\n",
        "        w2 = self.weights[-1]\n",
        "        #print(\"shape of weightss:\" , w2.shape)\n",
        "        # we get gradient of g1, by multiplying localGrad times weights, so we should get 30 x 1\n",
        "        grad_g1 = np.matmul(localGrad2, w2).transpose() # let's try\n",
        "\n",
        "        #print(\"SHAPE OF grad_g1: \", grad_g1.shape)\n",
        "        # now calculate the local grad 2\n",
        "        gradSigm1 = sigmoid_prime(fs[-2]).transpose() # 1 x 30\n",
        "        #print(\"grad g1 and GRAD SIGM2: \",grad_g1.shape, gradSigm1.shape)\n",
        "\n",
        "\n",
        "        localGrad1 = np.multiply(gradSigm1,grad_g1)\n",
        "\n",
        "        #print(\"LOCAL GRAD1 1: \", localGrad1.shape)\n",
        "\n",
        "        # ok now calculate the gradients for weights and bias\n",
        "\n",
        "        inputG = gs[-3]  # of shape 100x784\n",
        "        # localGrad1 is 100 x 30\n",
        "\n",
        "        grads_w_layer1 = np.matmul(localGrad1, inputG)\n",
        "        #print(\"HELLO\")\n",
        "        #print(\"GRADSw1: \", grads_w_layer1.shape)\n",
        "\n",
        "        grads_b_layer1 = np.sum(localGrad1, axis=-1)\n",
        "        #print(\"GRADsb1: \", grads_b_layer1.shape)\n",
        "\n",
        "        grads_w.append(grads_w_layer1)\n",
        "\n",
        "        grads_b.append(grads_b_layer1)\n",
        "\n",
        "        grads_w = grads_w[::-1]\n",
        "\n",
        "        grads_b = grads_b[::-1]\n",
        "\n",
        "        return grads_w, grads_b\n",
        "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "\n",
        "        assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
        "        B, N_last = a.shape\n",
        "        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n",
        "\n",
        "    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n",
        "        \"\"\"\n",
        "        Compute accuracy: the ratio of correct answers for test_data.\n",
        "\n",
        "        Args:\n",
        "        - x_test_data: shape (B, N^0).\n",
        "        - y_test_data: shape (B, N^last).\n",
        "        \"\"\"\n",
        "        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n",
        "        targets = np.argmax(y_test_data, axis=1)\n",
        "        return np.mean(predictions == targets)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
        "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
        "        epochs: int = 2,\n",
        "        mini_batch_size: int = 100,\n",
        "        learning_rate: float = 0.001\n",
        "    ) -> np.float64:\n",
        "        x_train, y_train = training_data\n",
        "        progress_bar = tqdm(range(epochs), desc=\"Epoch\")\n",
        "        for epoch in progress_bar:\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                i_begin = i * mini_batch_size\n",
        "                i_end = (i + 1) * mini_batch_size\n",
        "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
        "            if test_data:\n",
        "                x_test, y_test = test_data\n",
        "                accuracy = self.evaluate(x_test, y_test)\n",
        "                progress_bar.set_postfix_str(f\"Test accuracy: {accuracy * 100:.2f} %\")\n",
        "\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "            return self.evaluate(x_test, y_test)\n",
        "        else:\n",
        "            return np.float64(-1)\n",
        "\n",
        "model_results.evaluate_model(model_name=\"Baseline\", model_constructor=Network, n_trainings=3)\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "id": "tnBAMfMP6IRJ",
        "outputId": "9d64b6ee-2c7e-478c-83e4-afbb42815ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.11it/s, Test accuracy: 91.09 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it, Test accuracy: 91.15 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it, Test accuracy: 90.90 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.32it/s, Test accuracy: 94.84 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it, Test accuracy: 95.11 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 94.46 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.19it/s, Test accuracy: 8.92 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.01it/s, Test accuracy: 10.28 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it, Test accuracy: 12.06 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7b63b3da8fb0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_14e30\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_14e30_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_14e30_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_14e30_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_14e30_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_14e30_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_14e30_row0_col2\" class=\"data row0 col2\" >94.8% ± 0.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_14e30_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_14e30_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "      <td id=\"T_14e30_row1_col2\" class=\"data row1 col2\" >91.0% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_14e30_row2_col0\" class=\"data row2 col0\" >Baseline</td>\n",
              "      <td id=\"T_14e30_row2_col1\" class=\"data row2 col1\" >1e+02</td>\n",
              "      <td id=\"T_14e30_row2_col2\" class=\"data row2 col2\" >10.4% ± 1.3 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZIY72SXywD"
      },
      "source": [
        "## Task 1: softmax & cross-entropy loss\n",
        "Use softmax instead of coordinate-wise sigmoid and use negative-log-loss instead of MSE. Test to see if this improves convergence.   \n",
        "\n",
        "Hints:\n",
        "* When implementing backprop it's easier to consider these two functions as a single block, skipping the computation of the gradient over the softmax values, and going directly to gradients over logits (last pre-activations).\n",
        "* Softmax is only used after the last layer; previous layers (and their grad computations) can be unchanged.\n",
        "* Remember to update the forward pass in both places.\n",
        "* Loss for a mini-batch is the mean of losses for each dataitem in it, by convention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "77iGbiSDXywD",
        "outputId": "510cb977-66cc-4180-e417-9f5550686cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  40%|████      | 4/10 [01:00<01:30, 15.12s/it, Test accuracy: 78.62 %]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1972490090.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mmodel_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SoftMax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTask1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trainings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1281756613.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model_name, model_constructor, layers, learning_rates, n_trainings, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trainings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 accuracy = network.train(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2188505748.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, test_data, epochs, mini_batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mi_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mi_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class Task1(Network):\n",
        "    def __init__(self, sizes: Sequence[int]):\n",
        "        super().__init__(sizes=sizes)\n",
        "        self.l2_factor = 1e-5\n",
        "        self.momentum = 0.99\n",
        "        self.velocity0 = 0\n",
        "        self.velocity1 = 0\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        g1 = sigmoid(x @ self.weights[0].T + self.biases[0])\n",
        "        g = stable_softmax(g1 @ self.weights[1].T + self.biases[1])\n",
        "        #print(\"SHAPE OF SOFT: \", g.shape)\n",
        "        return g\n",
        "\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        ## TODO\n",
        "        ###{\n",
        "        ###}\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "\n",
        "        newW0 = self.weights[0]\n",
        "        newW1 = self.weights[1]\n",
        "\n",
        "        newB0 = self.biases[0]\n",
        "        newB1 = self.biases[1]\n",
        "\n",
        "        vel0 = self.velocity0\n",
        "        vel1 = self.velocity1\n",
        "\n",
        "        vel0 = self.momentum * vel0  + grads_w[0] + self.l2_factor * self.weights[0]\n",
        "        vel1 = self.momentum * vel1  + grads_w[1] + self.l2_factor * self.weights[1]\n",
        "\n",
        "        newW0 = self.weights[0] - learning_rate * vel0\n",
        "        newW1 = self.weights[1] - learning_rate * vel1\n",
        "\n",
        "\n",
        "        newB0 = self.biases[0] - learning_rate * grads_b[0]\n",
        "        newB1 = self.biases[1] - learning_rate * grads_b[1]\n",
        "\n",
        "        self.weights[0] = newW0\n",
        "        self.weights[1] = newW1\n",
        "\n",
        "        self.biases[0] = newB0\n",
        "        self.biases[1] = newB1\n",
        "\n",
        "    def feedTrain(self, x: FloatNDArray):\n",
        "        g = x\n",
        "        fs = []\n",
        "        gs = []\n",
        "\n",
        "        newF1 = np.matmul(self.weights[0],g.transpose()).transpose() + self.biases[0]\n",
        "\n",
        "        fs.append(newF1)\n",
        "\n",
        "        newG1 = sigmoid(newF1)\n",
        "\n",
        "        gs.append(newG1)\n",
        "\n",
        "        newF2 = np.matmul(self.weights[1],newG1.transpose()).transpose() + self.biases[1] # right before softmax\n",
        "\n",
        "        fs.append(newF2)\n",
        "        newG = stable_softmax(newF2)\n",
        "\n",
        "        gs.append(newG)\n",
        "\n",
        "        #for gi in gs:\n",
        "          #print(\"SHAPE OF GS: \", gi.shape)\n",
        "        return fs, gs\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "        #print(\"Y: \", y.shape)\n",
        "        # Forward pass.\n",
        "        # Activations (including input) of shapes (B, N^0), (B, N^1), ..., (B, N^last).\n",
        "\n",
        "        grads_w = []  # Shapes (N^last, N^{last-1}), ..., (N^1, N^0).\n",
        "        grads_b = []  # Shapes (N^last,), ..., (N^1,).\n",
        "\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "        fs,newgs = self.feedTrain(x)\n",
        "        gs += newgs\n",
        "        # now compute the gradients\n",
        "        # gradient local gradient for f2:\n",
        "\n",
        "        localGrad2 = gs[-1] - y\n",
        "\n",
        "\n",
        "        ##print(\"shape of local2: \", localGrad2.shape) # 100 x 10\n",
        "\n",
        "\n",
        "        w1Grad = localGrad2.T @ gs[-2]\n",
        "\n",
        "        b1Grad = np.sum(localGrad2.transpose(),axis=-1)\n",
        "\n",
        "        ##print(\"shape of w2 and b2 grads: \", w1Grad.shape, b1Grad.shape)\n",
        "        grads_w.append(w1Grad)\n",
        "        grads_b.append(b1Grad)\n",
        "\n",
        "        # now compute the other gradients\n",
        "\n",
        "        w2 = self.weights[-1]\n",
        "\n",
        "\n",
        "\n",
        "        gradG = np.matmul(localGrad2, w2) # let's try\n",
        "\n",
        "        #print(\"SHAPE OF GRAD AND SIGMOID: \", gradG.shape, sigmoid_prime(fs[-2]).shape)\n",
        "        localGrad1 = np.multiply(sigmoid_prime(fs[-2]),gradG)\n",
        "\n",
        "        grads_w_layer1 = np.matmul(localGrad1.transpose(), x)\n",
        "\n",
        "\n",
        "        grads_b_layer1 = np.sum(localGrad1.transpose(), axis=-1)\n",
        "\n",
        "        grads_w.append(grads_w_layer1)\n",
        "\n",
        "        grads_b.append(grads_b_layer1)\n",
        "\n",
        "        grads_w = grads_w[::-1]\n",
        "\n",
        "        grads_b = grads_b[::-1]\n",
        "\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "\n",
        "model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3,learning_rates=[1])\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvIk4RxTXywD"
      },
      "source": [
        "## Task 2: L2-regularization and momentum\n",
        "Implement L2-regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n",
        "A few notes:\n",
        "* do not regularize the biases\n",
        "* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "s3-03midXywD",
        "outputId": "3c9159c2-1b4e-4e93-f803-6d6fca44fe5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 5 random trainings with with lr = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/10 [00:06<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1093249334.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m model_results.evaluate_model(\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"L2&Momentum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1281756613.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model_name, model_constructor, layers, learning_rates, n_trainings, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trainings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 accuracy = network.train(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2188505748.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, test_data, epochs, mini_batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mi_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mi_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2188505748.py\u001b[0m in \u001b[0;36mlearning_step\u001b[0;34m(self, x_mini_batch, y_mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Gradient descent step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2188505748.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m#print(\"GRADSw1: \", grads_w_layer1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mgrads_b_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalGrad1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;31m#print(\"GRADsb1: \", grads_b_layer1.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m     return _wrapreduction(\n\u001b[0m\u001b[1;32m   2390\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m         \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[1;32m     71\u001b[0m                   if v is not np._NoValue}\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class Task2(Network):\n",
        "    def __init__(\n",
        "        self, sizes: Sequence[int], l2_factor: float = 1e-5, momentum: float = 0.99\n",
        "    ):\n",
        "        super().__init__(sizes=sizes)\n",
        "        self.l2_factor = l2_factor\n",
        "        self.momentum = momentum\n",
        "        self.velocity0 = 0\n",
        "        self.velocity1 = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        newW0 = self.weights[0]\n",
        "        newW1 = self.weights[1]\n",
        "\n",
        "        newB0 = self.biases[0]\n",
        "        newB1 = self.biases[1]\n",
        "\n",
        "        vel0 = self.velocity0\n",
        "        vel1 = self.velocity1\n",
        "\n",
        "        vel0 = self.momentum * vel0  + grads_w[0] + self.l2_factor * self.weights[0]\n",
        "        print(\"VELS: \", vel0, vel1)\n",
        "        vel1 = self.momentum * vel1  + grads_w[1] + self.l2_factor * self.weights[1]\n",
        "\n",
        "        newW0 = self.weights[0] - learning_rate * vel0\n",
        "        newW1 = self.weights[1] - learning_rate * vel1\n",
        "\n",
        "\n",
        "        newB0 = self.biases[0] - learning_rate * grads_b[0]\n",
        "        newB1 = self.biases[1] - learning_rate * grads_b[1]\n",
        "\n",
        "\n",
        "        self.weights[0] = newW0\n",
        "        self.weights[1] = newW1\n",
        "\n",
        "        self.biases[0] = newB0\n",
        "        self.biases[1] = newB1\n",
        "\n",
        "        self.velocity0 = vel0\n",
        "        self.velocity1 = vel1\n",
        "\n",
        "\n",
        "def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "        #print(\"Y: \", y.shape)\n",
        "        # Forward pass.\n",
        "        # Activations (including input) of shapes (B, N^0), (B, N^1), ..., (B, N^last).\n",
        "\n",
        "        grads_w = []  # Shapes (N^last, N^{last-1}), ..., (N^1, N^0).\n",
        "        grads_b = []  # Shapes (N^last,), ..., (N^1,).\n",
        "\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "        fs,newgs = self.feedTrain(x)\n",
        "        gs += newgs\n",
        "        # now compute the gradients\n",
        "        # gradient local gradient for f2:\n",
        "\n",
        "        localGrad2 = gs[-1] - y\n",
        "        ##print(\"shape of local2: \", localGrad2.shape) # 100 x 10\n",
        "\n",
        "        localGrad2 /= B\n",
        "\n",
        "        w1Grad = localGrad2.T @ gs[-2]\n",
        "\n",
        "        b1Grad = np.sum(localGrad2.transpose(),axis=-1)\n",
        "\n",
        "        ##print(\"shape of w2 and b2 grads: \", w1Grad.shape, b1Grad.shape)\n",
        "        grads_w.append(w1Grad)\n",
        "        grads_b.append(b1Grad)\n",
        "\n",
        "        # now compute the other gradients\n",
        "\n",
        "        w2 = self.weights[-1]\n",
        "\n",
        "\n",
        "\n",
        "        gradG = np.matmul(localGrad2, w2) # let's try\n",
        "\n",
        "        #print(\"SHAPE OF GRAD AND SIGMOID: \", gradG.shape, sigmoid_prime(fs[-2]).shape)\n",
        "        localGrad1 = np.multiply(sigmoid_prime(fs[-2]),gradG)\n",
        "\n",
        "        grads_w_layer1 = np.matmul(localGrad1.transpose(), x)\n",
        "\n",
        "\n",
        "        grads_b_layer1 = np.sum(localGrad1.transpose(), axis=-1)\n",
        "\n",
        "        grads_w.append(grads_w_layer1)\n",
        "\n",
        "        grads_b.append(grads_b_layer1)\n",
        "\n",
        "        grads_w = grads_w[::-1]\n",
        "\n",
        "        grads_b = grads_b[::-1]\n",
        "\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"L2&Momentum\",\n",
        "    model_constructor=Task2,\n",
        "    learning_rates=[100],\n",
        "    n_trainings=5,\n",
        "    l2_factor=1e-5,\n",
        "    momentum=0.99\n",
        ")\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1And2(Task2, Task1):\n",
        "    # A somewhat hacky but short way to mix Task1 and Task2.\n",
        "    # You could also just replace the superclass of Task2 to be Task1.\n",
        "    pass\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"Softmax&L2&Momentum\",\n",
        "    model_constructor=Task1And2,\n",
        "    learning_rates=[2.0],\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1\n",
        ")\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "id": "nnBGG1xo0F34",
        "outputId": "435cf5f2-5063-4a95-8b99-475674cce4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-755551897.py:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  10%|█         | 1/10 [00:00<00:08,  1.06it/s, Test accuracy: 9.58 %]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  10%|█         | 1/10 [00:01<00:12,  1.41s/it, Test accuracy: 9.58 %]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n",
            "HHH\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2913980918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model_results.evaluate_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Softmax&L2&Momentum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTask1And2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3806505274.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model_name, model_constructor, layers, learning_rates, n_trainings, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trainings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 accuracy = network.train(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2188505748.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, test_data, epochs, mini_batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mi_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mi_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2614096299.py\u001b[0m in \u001b[0;36mlearning_step\u001b[0;34m(self, x_mini_batch, y_mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m###{\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m###}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2614096299.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mlocalGrad1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgradG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mgrads_w_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalGrad1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nLauKUXywE"
      },
      "source": [
        "## Task 3 (optional)\n",
        "Implement more variations of SGD:\n",
        "* AdamW (probably the most popular choice) or Adagrad,\n",
        "* dropout\n",
        "* some simple data augmentations (e.g. tiny rotations/shifts etc.).\n",
        "\n",
        "Again, test to see how these changes improve accuracy/convergence.  \n",
        "\n",
        "Quick reminders:\n",
        "* for AdamW, check the official [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)'s pseudocode or the original paper: [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).\n",
        "* for AdaGrad, check the Appendix of this notebook.\n",
        "* for dropout: during training only, zero-out each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X3hRIizXywE"
      },
      "outputs": [],
      "source": [
        "# Place for remaining parts of task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HCbwiW2XywE"
      },
      "source": [
        "## Task 4\n",
        "Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence and what learning rates work.\n",
        "\n",
        "As a start, you can try this slightly larger architecture: [784,100,30,10]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92OPX1uCXywE"
      },
      "outputs": [],
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xl3A1WSXywE"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRX8ith-XywF"
      },
      "source": [
        "## Adagrad (simplified version)\n",
        "\n",
        "Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n",
        "For parameter $p_i$ we maintain a variable $G_i$ (can be set to $0$ initially).\n",
        "Let $\\mathcal{L}$ be our loss without L2.   \n",
        "We update $G_i$ and $p_i$ each training step as follows:  \n",
        "$$\n",
        "G_i = G_i +  \\left(\\frac{\\partial \\mathcal{L}}{\\partial p_i}\\right)^2\\\\\n",
        "p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\mathcal{L}}{\\partial p_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfBEYR4rYXmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}