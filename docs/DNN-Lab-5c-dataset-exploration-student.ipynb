{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this task you will use modified MNIST dataset where some images are randomly relabelled.\n",
        "You are given a pipeline that trains a simple convolutional neural network on this dataset.\n",
        "The purpose of this task is to show you how to create data visualizations (e.g. by creating image embeddings) and use them to identify problems with data like incorrect labels.\n",
        "\n",
        "Tasks:\n",
        "1. Run training and check that the model achieves accuracy below 70% on the test set.\n",
        "2. Use clusterfun visualizations to identify mislabelled elements of the train dataset.\n",
        "3. Relabel these elements - make the labels correct (use provided `FashionMNISTWithRandomModifications.relabel()` method).\n",
        "4. Train the model once again on the updated train data. With correct labels it should achieve accuracy above 95% on the test set.\n"
      ],
      "metadata": {
        "id": "eYSkBF_xVvra"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gK11fqpP4UXG",
        "outputId": "855e2631-7d66-4c44-aec9-70565de2bf98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q clusterfun umap-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import umap\n",
        "import clusterfun as clt\n",
        "from PIL import Image\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE"
      ],
      "metadata": {
        "id": "Z6viK49Z5cio"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "\n",
        "def show_port(port, height=700):\n",
        "    \"\"\"\n",
        "    Helper function for displaying iframe with clusterfun interactive visualizations.\n",
        "    \"\"\"\n",
        "    display(Javascript(\"\"\"\n",
        "        (async ()=>{\n",
        "            fm = document.createElement('iframe')\n",
        "            fm.src = await google.colab.kernel.proxyPort(%s)\n",
        "            fm.width = '95%%'\n",
        "            fm.height = '%d'\n",
        "            fm.frameBorder = 0\n",
        "            fm.style.background = 'white'\n",
        "            fm.scrolling = 'no'\n",
        "            document.body.append(fm)\n",
        "        })();\n",
        "    \"\"\" % (port, height) ))"
      ],
      "metadata": {
        "id": "s3ODq0lpBlUl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv_layers = torch.nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(0.2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Dropout2d(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        self.head = torch.nn.Sequential(\n",
        "            nn.Linear(7 * 7 * 32, 200),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MNISTWithRandomModifications(MNIST):\n",
        "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
        "        super(MNISTWithRandomModifications, self).__init__(root, train, transform, target_transform, download)\n",
        "        self._extract_images()\n",
        "        self.dataframe = self._create_dataframe()\n",
        "        if train:\n",
        "            self._change_labels_randomly()\n",
        "\n",
        "    def _extract_images(self):\n",
        "        if not os.path.exists(self._img_folder):\n",
        "            os.makedirs(self._img_folder)\n",
        "            for i, img in enumerate(self.data):\n",
        "                img_path = os.path.join(self._img_folder, f'img{i}.png')\n",
        "                Image.fromarray(img.numpy()).save(img_path)\n",
        "\n",
        "    def _change_labels_randomly(self):\n",
        "        images_per_label = len(self) // 10\n",
        "        # For simplicity we randomly change labels in continuous segments based on _id.\n",
        "        # Each segment contains samples with the same label and a new label is also\n",
        "        # identical for all elements of the segment.\n",
        "        num_random_segments = 3\n",
        "        random_segment_length = 5000\n",
        "\n",
        "        random_labels = np.random.choice(10, num_random_segments, replace=False)\n",
        "        new_labels = np.zeros(num_random_segments).astype(np.int64)\n",
        "        for i, label in enumerate(random_labels):\n",
        "            possible_new_labels = list(set(range(10)) - {label})\n",
        "            new_labels[i] = np.random.choice(possible_new_labels, 1).item()\n",
        "\n",
        "        segment_start_ids = np.zeros(num_random_segments).astype(np.int64)\n",
        "        for i, random_label in enumerate(random_labels):\n",
        "            label_ids = self.dataframe[self.dataframe['label'] == random_label]['_id']\n",
        "            num_elements_with_label = max(label_ids) - min(label_ids)\n",
        "            random_idx = np.random.choice(num_elements_with_label - random_segment_length, 1).item()\n",
        "            segment_start_ids[i] = min(label_ids) + random_idx\n",
        "\n",
        "        segment_end_ids = segment_start_ids + random_segment_length\n",
        "        relabel_ids = np.concatenate([\n",
        "            np.arange(start, end)\n",
        "            for start, end in zip(segment_start_ids, segment_end_ids)\n",
        "        ])\n",
        "        relabel_indices = self.dataframe[self.dataframe['_id'].isin(relabel_ids)].index\n",
        "        new_labels_repeated = new_labels.repeat(random_segment_length)\n",
        "        self.dataframe.loc[relabel_indices, 'label'] = new_labels_repeated\n",
        "        self.targets[relabel_indices] = torch.tensor(new_labels_repeated)\n",
        "\n",
        "    def _create_dataframe(self):\n",
        "        df = pd.DataFrame({\n",
        "            'img_path': [\n",
        "                os.path.abspath(os.path.join(self._img_folder, f'img{i}.png'))\n",
        "                for i in range(len(self))\n",
        "            ],\n",
        "            'label': self.targets,\n",
        "            'pred': pd.NA,\n",
        "        })\n",
        "        # Reorder data so that observations with the same label are placed in continuous\n",
        "        # segments with respect to their _id.\n",
        "        # E.g. samples with label 0 have ids 0-5999, with label 1 have ids 6000-11999 etc.\n",
        "        # This is needed to make relabelling exercise simple.\n",
        "        # Note that we do not change dataframe index. Therefore to find element with given id\n",
        "        # in self.data we need to do:\n",
        "        # self.data[self.dataframe[self.dataframe['_id' = id]].index.item()].\n",
        "        df = df.sort_values('label')\n",
        "        df['_id'] = range(len(self))\n",
        "        return df\n",
        "\n",
        "    @property\n",
        "    def _img_folder(self):\n",
        "        return os.path.join(\n",
        "            self.root,\n",
        "            self.__class__.__name__,\n",
        "            'img',\n",
        "            'train' if self.train else 'test'\n",
        "        )\n",
        "\n",
        "    def relabel(self, ids: list[int], new_labels: list[int]):\n",
        "        \"\"\"\n",
        "        Use this function to relabel data after identifying elements with incorrect labels.\n",
        "        :param ids: Values of '_id' column of elements from the dataset that should be relabelled.\n",
        "        :param new_labels: New labels for selected dataset elements. Labels must be integers in range 0-9.\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(ids) == len(new_labels), 'There should be exactly one label for each id'\n",
        "        relabel_indices = self.dataframe[self.dataframe['_id'].isin(ids)].index\n",
        "        self.targets[relabel_indices] = torch.tensor(new_labels)\n",
        "        self.dataframe.loc[relabel_indices, 'label'] = new_labels\n",
        "\n",
        "    def update_pred_in_dataframe(self, batch_idx: int, batch_size: int, pred: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Helper function for updating pred in dataframe stored by dataset.\n",
        "        :param pred: Tensor 1d with predicted labels. Labels must be integers in range 0-9.\n",
        "        \"\"\"\n",
        "        assert len(pred.shape) == 1, 'pred should be 1d tensor'\n",
        "        df_indices = range(\n",
        "            batch_idx * batch_size,\n",
        "            # Last batch may be smaller than batch_size if len(self) % batch_size != 0\n",
        "            min((batch_idx + 1) * batch_size, len(self))\n",
        "        )\n",
        "        train_loader.dataset.dataframe.loc[df_indices, 'pred'] = pred.numpy()\n",
        "\n",
        "    def generate_embeddings(self, seed=1):\n",
        "        \"\"\"\n",
        "        Generetes embeddings of images using UMAP.\n",
        "        \"\"\"\n",
        "        umap_reducer = umap.UMAP(random_state=seed)\n",
        "        umap_res = umap_reducer.fit_transform(\n",
        "            ((self.data - self.data.min()) / self.data.max()).view(len(self), -1).numpy()\n",
        "        )\n",
        "        assert umap_res.shape == (len(self), 2), f'Incorrect UMAP result shape: {umap_res.shape}'\n",
        "        self.dataframe.loc[range(len(self)), 'x'] = umap_res[:, 0]\n",
        "        self.dataframe.loc[range(len(self)), 'y'] = umap_res[:, 1]"
      ],
      "metadata": {
        "id": "wQR4WNEE9G3B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        pred = output.argmax(dim=1)\n",
        "        train_loader.dataset.update_pred_in_dataframe(batch_idx, train_loader.batch_size, pred)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            _, _, image_width, image_height = data.size()\n",
        "            print(\n",
        "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100.0 * batch_idx / len(train_loader),\n",
        "                    loss.item(),\n",
        "                )\n",
        "            )\n",
        "    print(\n",
        "        \"Train accuracy: {}/{} ({:.0f}%)\".format(\n",
        "            correct, len(train_loader.dataset),\n",
        "            100.0 * correct / len(train_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            test_loader.dataset.update_pred_in_dataframe(batch_idx, test_loader.batch_size, pred)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss, correct, len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "R1HRgHN_MOqT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "test_batch_size = 1000\n",
        "epochs = 5\n",
        "lr = 1e-3\n",
        "use_cuda = True\n",
        "seed = 1\n",
        "log_interval = 20"
      ],
      "metadata": {
        "id": "txLJOIMxKkgw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = use_cuda and torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "train_kwargs = {\"batch_size\": batch_size}\n",
        "test_kwargs = {\"batch_size\": test_batch_size}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)"
      ],
      "metadata": {
        "id": "v6TMJmYPKjOr"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"../data\"\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = MNISTWithRandomModifications(\n",
        "    data_dir,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "test_dataset = MNISTWithRandomModifications(\n",
        "    data_dir,\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, **train_kwargs)\n",
        "test_loader = DataLoader(test_dataset, **test_kwargs)"
      ],
      "metadata": {
        "id": "cp2PM7XE5c7L"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch, log_interval)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "id": "KlUeSk6S7FpR",
        "outputId": "1c67d7a3-1ee7-4768-f4a5-ef6e8fdc0356",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.433308\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.536418\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.489771\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.298852\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.287279\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.157167\n",
            "Train accuracy: 51764/60000 (86%)\n",
            "Test set: Average loss: 0.5867, Accuracy: 7887/10000 (79%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.442074\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.133764\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.242834\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.166977\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.193741\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.100424\n",
            "Train accuracy: 56490/60000 (94%)\n",
            "Test set: Average loss: 0.5096, Accuracy: 7970/10000 (80%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.379680\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.095911\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.193189\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.109249\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.149229\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.081453\n",
            "Train accuracy: 57056/60000 (95%)\n",
            "Test set: Average loss: 0.4775, Accuracy: 7987/10000 (80%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.361811\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.078743\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.166231\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.099557\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.133425\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.085220\n",
            "Train accuracy: 57334/60000 (96%)\n",
            "Test set: Average loss: 0.4569, Accuracy: 8014/10000 (80%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.346336\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.066372\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.161023\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.080596\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.127509\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.079741\n",
            "Train accuracy: 57430/60000 (96%)\n",
            "Test set: Average loss: 0.4471, Accuracy: 8012/10000 (80%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset exploration\n",
        "\n",
        "You can notice that accuracy achieved on the test data is not very high. This is caused by incorrect labels assigned to some of the images. To identify mislabelled elements we will visualize data.\n",
        "\n",
        "In order to visualize data we need to generate embeddings. You can do this just by calling `FashionMNISTWithRandomModifications.generate_embeddings()` method. It generates embeddings with UMAP which is one of the techniques used for dimensionality reduction. (Other techniques commonly used in such cases include PCA or t-SNE.)\n",
        "\n",
        "For visualizations we will use [clusterfun](https://clusterfun.app/) library. It allows us to create a few interactive visualizations. You can find more details in documentation on their website. For visualization of embeddings we will use scatter plot.\n",
        "\n",
        "Because clusterfun does not support Colab we use small helper function `show_port()` to display plots. You need to copy port number printed by clusterfun and pass it to this function.\n",
        "\n",
        "Explore embeddings visualization and try to find mislabelled examples.\n",
        "\n",
        "Hint: To make this scenario simple we randomly change labels in 3 continuous segments based on _id. Each segment contains 5000 samples with the same label and a new label is also identical for all elements of the segment.\n",
        "\n"
      ],
      "metadata": {
        "id": "bnyZrijqfkZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.generate_embeddings(seed=seed)"
      ],
      "metadata": {
        "id": "XpMVDcstdw7U",
        "outputId": "c3a4517a-77b5-44e8-f017-5a980104d5d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clt.scatter(train_dataset.dataframe, x='x', y='y', color='label', media='img_path')"
      ],
      "metadata": {
        "id": "yTM-nzsfPxNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58056ce0-194a-44a9-cd79-d0404c706166"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serving plot on http://localhost:54957\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.cache/clusterfun/642159be-4202-4bbd-9e24-2c3a1dc3e6b9')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy port number from string printed above\n",
        "# Serving plot on http://localhost:<port_number>\n",
        "port_number = 54957\n",
        "show_port(port_number)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "FnRDiI6nYm_4",
        "outputId": "855bb707-0de9-4362-9b2a-985d059f0774"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async ()=>{\n",
              "            fm = document.createElement('iframe')\n",
              "            fm.src = await google.colab.kernel.proxyPort(54957)\n",
              "            fm.width = '95%'\n",
              "            fm.height = '700'\n",
              "            fm.frameBorder = 0\n",
              "            fm.style.background = 'white'\n",
              "            fm.scrolling = 'no'\n",
              "            document.body.append(fm)\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify and relabel data\n",
        "\n",
        "Then check how visualization changed."
      ],
      "metadata": {
        "id": "yPpyOw4Zj6dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## TODO #############\n",
        "# Identify incorrect labels on visualizations above and change them.\n",
        "# Use train_dataset.relabel()\n",
        "listFor5s = list(range(50000,55000))\n",
        "fives = [9] * 5000\n",
        "train_dataset.relabel(listFor5s,fives)\n",
        "# We see immeadietly, that a lot of samples of digit 9 are incorrectly labeled as 1, a lot of digit 5 are marked as 6"
      ],
      "metadata": {
        "id": "8-3B_G70ZOF0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## TODO #############\n",
        "# Visualize data after changes\n",
        "# You DO NOT need to recompute embeddings - images are the same, only labels changed\n",
        "clt.scatter(train_dataset.dataframe, x='x', y='y',color='label',media='img_path')"
      ],
      "metadata": {
        "id": "LibmqxI1mNF3",
        "outputId": "806f7f11-959f-4962-c9f3-eace2f8ce672",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serving plot on http://localhost:34937\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.cache/clusterfun/3dea40e6-328a-4dc5-96e2-6b747ff2abd6')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_port(port_number)"
      ],
      "metadata": {
        "id": "RzXAtbAzmOfr",
        "outputId": "89cf8d8e-2f08-4193-b484-bacff7222f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        (async ()=>{\n",
              "            fm = document.createElement('iframe')\n",
              "            fm.src = await google.colab.kernel.proxyPort(54957)\n",
              "            fm.width = '95%'\n",
              "            fm.height = '700'\n",
              "            fm.frameBorder = 0\n",
              "            fm.style.background = 'white'\n",
              "            fm.scrolling = 'no'\n",
              "            document.body.append(fm)\n",
              "        })();\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrain the model\n",
        "\n",
        "After changing labels to the correct ones the model should achieve above 95% accuracy on the test set."
      ],
      "metadata": {
        "id": "pRhvC9Qbj_qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## TODO #############\n",
        "# Train model using changed data"
      ],
      "metadata": {
        "id": "ZwutWAncOGhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MgFbz4XakNQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}