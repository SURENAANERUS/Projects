{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURENAANERUS/Projects/blob/master/docs/hw_1_multitask_counting_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Multitask Learning for Geometric Shape Classification and Counting**\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "In this project, you will design, implement, and evaluate a **multitask neural network** that performs **two tasks simultaneously**:\n",
        "\n",
        "1. **Classification** – identify which pair of geometric shape types appears in a 28×28 binary image (135 possible configurations).\n",
        "2. **Regression** – predict how many shapes of each type are present (6 regression targets).\n",
        "\n",
        "This project focuses on **multi-task learning**, i.e., using one shared model to learn several related tasks at once. You will compare how adding an auxiliary task affects performance and training dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset\n",
        "\n",
        "You will use the **Geometric Shape Numbers (GSN)** dataset:\n",
        "\n",
        "```bash\n",
        "!wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip\n",
        "```\n",
        "\n",
        "This will create a directory `data/` containing:\n",
        "\n",
        "* **10,000 images** (28×28x1, grayscale)\n",
        "* **labels.csv** – counts of each of six shape types per image\n",
        "\n",
        "Each image contains exactly **two types** of geometric figures (out of six) and **10 shapes total**.\n",
        "\n",
        "**Shape classes:**\n",
        "\n",
        "| Index | Shape type     |\n",
        "| ----: | -------------- |\n",
        "|     0 | square         |\n",
        "|     1 | circle         |\n",
        "|     2 | triangle up    |\n",
        "|     3 | triangle right |\n",
        "|     4 | triangle down  |\n",
        "|     5 | triangle left  |\n",
        "\n",
        "Example row from `labels.csv`:\n",
        "\n",
        "```\n",
        "name,squares,circles,up,right,down,left\n",
        "img_00000.png,0,0,0,4,0,6\n",
        "```\n",
        "\n",
        "Here, the image contains **4 right-pointing triangles** and **6 left-pointing triangles**.\n",
        "\n",
        "**Split:**\n",
        "\n",
        "* Training: first 9,000 samples\n",
        "* Validation: last 1,000 samples\n",
        "\n",
        "Examples:\n",
        "![example.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Tasks and objectives\n",
        "\n",
        "You must design a **multitask deep learning system** that:\n",
        "\n",
        "1. **Classifies** each image into one of **135 possible configurations**, representing:\n",
        "\n",
        "   * which **two shape classes** appear, and\n",
        "   * how their counts (1–9) sum to 10.\n",
        "\n",
        "   → Example: \"3 circles + 7 squares\" is one configuration class.\n",
        "\n",
        "2. **Regresses** the number of shapes of each type (a 6-dimensional real-valued output).\n",
        "\n",
        "3. Combines both objectives in a **joint loss** function (Hint: losses are implemented in PyTorch):\n",
        "\n",
        "\n",
        "$$ Loss = \\text{NLLLoss(classification)} + \\lambda_{\\text{cnt}} \\cdot \\text{SmoothL1Loss(regression)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model requirements\n",
        "\n",
        "### Architecture constraints\n",
        "\n",
        "You must use **exactly this feature extractor (backbone)**:\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        ")\n",
        "```\n",
        "\n",
        "Then add **two separate heads**:\n",
        "\n",
        "* `head_cls`: outputs log-probabilities for 135 classes\n",
        "* `head_cnt`: outputs 6 regression values (counts)\n",
        "\n",
        "The model must return two outputs: `(log_probs, counts)`.\n",
        "\n",
        "You may add dropout or batch normalization inside the heads, **but you must not modify the backbone**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Training setup\n",
        "\n",
        "* Optimizer: **Adam**, learning rate = 1e-3\n",
        "* Epochs: up to **100** (use **early stopping**)\n",
        "* Batch sizes: **64** (train), **1000** (validation)\n",
        "* Device: GPU allowed for Notebook, but your **final code must run on GPU within ~30 minutes**\n",
        "* Random seed: set `torch.manual_seed(1)` for reproducibility\n",
        "* Split: **exactly 9,000 train / 1,000 validation**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Data preprocessing and augmentation\n",
        "\n",
        "You must implement a **PyTorch `Dataset` class** that:\n",
        "\n",
        "* Reads `labels.csv`\n",
        "* Loads the corresponding image (from `data/`)\n",
        "* Returns both:\n",
        "  * the image (as a tensor)\n",
        "  * the labels (counts for 6 shapes)\n",
        "* Optionally applies transformations\n",
        "\n",
        "### Required augmentations\n",
        "\n",
        "You must implement **at least three** of the following:\n",
        "\n",
        "1. Random horizontal flip\n",
        "2. Random vertical flip\n",
        "3. Random 90° rotation (must correctly rotate orientation labels: up → right → down → left)\n",
        "4. Random brightness/contrast (mild)\n",
        "5. Gaussian noise\n",
        "6. Random erasing (small areas only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation metrics\n",
        "\n",
        "Implement and report the following metrics on the validation set:\n",
        "\n",
        "### (a) **Classification (135-way)**\n",
        "\n",
        "* Top-1 accuracy\n",
        "* Macro F1-score\n",
        "* Per-pair accuracy (aggregate by unordered shape pair, e.g. {circle, up})\n",
        "\n",
        "### (b) **Regression (6-D counts)**\n",
        "\n",
        "* RMSE per class and overall\n",
        "* MAE per class and overall\n",
        "\n",
        "Also plot:\n",
        "\n",
        "* Training and validation losses\n",
        "* Validation accuracy and RMSE over epochs\n",
        "\n",
        "**Important**: This task is not about finding the best architecture; we expect at least 50% accuracy, but achieving results higher than that will not affect the grade for the assignment**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Experiments and analysis\n",
        "\n",
        "You must train and compare **three model settings**:\n",
        "\n",
        "| Setting | Description                                      |\n",
        "| :------ | :----------------------------------------------- |\n",
        "| 1       | **Classification-only:** λ_cnt = 0               |\n",
        "| 2       | **Regression-only:** classification loss ignored |\n",
        "| 3       | **Multitask:** λ_cnt = with your choose          |\n",
        "\n",
        "For each experiment:\n",
        "\n",
        "* Train until early stopping\n",
        "* Record loss, accuracy, RMSE, and runtime\n",
        "* Compare results and explain how λ influences learning\n",
        "* Discuss whether multitask learning improves the main tasks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Final deliverables\n",
        "\n",
        "You must submit .zip project with:\n",
        "\n",
        "1. **Code** (`.ipynb` or `.py`) that:\n",
        "\n",
        "   * Downloads and extracts the dataset\n",
        "   * Defines dataset, dataloaders, model, loss, training loop, evaluation, and plotting\n",
        "   * Can run start-to-end without interaction, and finishes within 30 minutes on Colab T4 GPUs\n",
        "   * Includes three experiment configurations\n",
        "\n",
        "2. **Report (2–4 pages, PDF)** including:\n",
        "   * Section on (EDA) Exploratory Data Analysis in your report: no more than 3 graphs or tables describing the data set.\n",
        "   * Model architecture\n",
        "   * Description and justification of augmentations\n",
        "   * Results table (loss, accuracy, RMSE for all runs)\n",
        "   * Learning curves\n",
        "   * Discussion on multitask effects\n",
        "\n",
        "3. **README.md**:\n",
        "\n",
        "   * Link to Colab version of task for fast replication.\n",
        "   * Approximate runtime and resource requirements\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Grading rubric\n",
        "\n",
        "Component\tDescription\tPoints\n",
        "1. Implementation correctness\tCorrect use of the fixed backbone, two-headed model, and proper training loop (classification + regression).\t30%\n",
        "2. Data & augmentations\tProper dataset loading, preprocessing, and at least three augmentations with brief justification.\t20%\n",
        "3. Evaluation & experiments\tCorrect computation of metrics (accuracy, F1, RMSE) and completion of all three λ configurations (λ=0, regression-only, your choice λ).\t30%\n",
        "4. Report & analysis\n",
        "A clear separation of concerns (e.g. headers in notebooks, modules in code) and concise 2–4 page report with results tables, learning curves, confusion matrix, and short discussion on multitask effects and error examples.\n",
        "20%\n",
        "\n",
        "###### Readability and modularity will be considered within each grading component. Clear structure (headers in notebooks, docstrings, modular code) significantly improves evaluation speed. Emphasize using clear headers to help reviewers navigate efficiently.\n",
        "---"
      ],
      "metadata": {
        "id": "_NvRrg8YvTPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmggC-uvcGR",
        "outputId": "b16fb2ab-fcfa-4bc0-d452-028d1d274a1e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-25 21:22:39--  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip [following]\n",
            "--2025-11-25 21:22:39--  https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5544261 (5.3M) [application/zip]\n",
            "Saving to: ‘data_gsn.zip’\n",
            "\n",
            "data_gsn.zip        100%[===================>]   5.29M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-25 21:22:39 (91.9 MB/s) - ‘data_gsn.zip’ saved [5544261/5544261]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and datahelpers"
      ],
      "metadata": {
        "id": "IFkkM0z-45pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomHorizontalFlip(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, label: np.ndarray):\n",
        "        imgNew = img.copy()\n",
        "        labelNew = label.copy()\n",
        "        if torch.rand(1) < self.p:\n",
        "            imgNew =  T.functional.hflip(imgNew)\n",
        "            placeHold = labelNew[5]\n",
        "            labelNew[5] = labelNew[3]\n",
        "            labelNew[3] = placeHold\n",
        "\n",
        "        return imgNew, labelNew\n",
        "\n",
        "\n",
        "class RandomVerticalFlip(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, label: np.ndarray):\n",
        "        imgNew = img.copy()\n",
        "        labelNew = label.copy()\n",
        "        if torch.rand(1) < self.p:\n",
        "\n",
        "            imgNew =  T.functional.vflip(imgNew)\n",
        "            placeHold = labelNew[2]\n",
        "            labelNew[2] = labelNew[4]\n",
        "            labelNew[4] = placeHold\n",
        "\n",
        "        return imgNew, labelNew\n",
        "\n",
        "class Random90DegFlip(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, label: np.ndarray):\n",
        "        imgNew = img.copy()\n",
        "        labelNew = label.copy()\n",
        "        if torch.rand(1) < self.p:\n",
        "            imgNew =  T.functional.rotate(imgNew,90)\n",
        "            plcHoldUp = labelNew[2]\n",
        "            plcHolLeft = labelNew[5]\n",
        "            plcHolDown = labelNew[4]\n",
        "            plcHolRight = labelNew[3]\n",
        "\n",
        "            labelNew[2] = plcHolLeft # 2 becomes 5\n",
        "            labelNew[3] = plcHoldUp # 3 becomes 2\n",
        "            labelNew[4] = plcHolRight # 4 becomes 3\n",
        "            labelNew[5] = plcHolDown # 5 becomes 4\n",
        "        return imgNew, labelNew\n",
        "\n",
        "\n",
        "indexDict = {\n",
        "    \"01\":0,\n",
        "    \"02\":1,\n",
        "    \"03\":2,\n",
        "    \"04\":3,\n",
        "    \"05\":4,\n",
        "    \"12\":5,\n",
        "    \"13\":6,\n",
        "    \"14\":7,\n",
        "    \"15\":8,\n",
        "    \"23\":9,\n",
        "    \"24\":10,\n",
        "    \"25\":11,\n",
        "    \"34\":12,\n",
        "    \"35\":13,\n",
        "    \"45\":14\n",
        "}\n",
        "\n",
        "valDict = {\n",
        "    \"19\":0,\n",
        "    \"28\":1,\n",
        "    \"37\":2,\n",
        "    \"46\":3,\n",
        "    \"55\":4,\n",
        "    \"64\":5,\n",
        "    \"73\":6,\n",
        "    \"82\":7,\n",
        "    \"91\":8,\n",
        "}\n",
        "\n",
        "pairDict = {\n",
        "    0:\"square circle\",\n",
        "    1:\"square up\",\n",
        "    2:\"square right\",\n",
        "    3:\"square down\",\n",
        "    4:\"square left\",\n",
        "    5:\"circle up\",\n",
        "    6:\"circle right\",\n",
        "    7:\"circle down\",\n",
        "    8:\"circle left\",\n",
        "    9:\"up right\",\n",
        "    10:\"up down\",\n",
        "    11:\"up left\",\n",
        "    12:\"right down\",\n",
        "    13:\"right left\",\n",
        "    14:\"down left\"\n",
        "\n",
        "}\n",
        "\n",
        "def listToNum(y):\n",
        "  # here y is [0,0,3,7,0,0] for example\n",
        "  val1,val2,i1,i2 = -1,-1,-1,-1\n",
        "  for i in range(6):\n",
        "    if y[i] != 0:\n",
        "      if val1 == -1:\n",
        "        val1 = y[i]\n",
        "        i1 = i\n",
        "      else:\n",
        "        val2 = y[i]\n",
        "        i2 = i\n",
        "  ixPair = str(int(i1)) + str(int(i2))\n",
        "  ixVal = indexDict[ixPair]\n",
        "\n",
        "  valPair = str(int(val1)) + str(int(val2))\n",
        "  valVal = valDict[valPair]\n",
        "  finalIx = ixVal * 9 + valVal\n",
        "\n",
        "\n",
        "  return finalIx\n",
        "\n",
        "def numToPair(y):\n",
        "  valIndex = y % 9\n",
        "  ixIndex = (y - valIndex) / 9\n",
        "\n",
        "\n",
        "  return int(ixIndex)\n",
        "\n",
        "def aggregateAccuraciesPerPair(TP,FP,TN,FN):\n",
        "  pTP,pFP,pTN,pFN = [0]*15,[0]*15,[0]*15,[0]*15\n",
        "  pairAccuracies = [0] *15\n",
        "  for i in range(135):\n",
        "    pairIndex = numToPair(i)\n",
        "    pTP[pairIndex] += TP[i]\n",
        "    pFP[pairIndex] += FP[i]\n",
        "    pTN[pairIndex] += TN[i]\n",
        "    pFN[pairIndex] += FN[i]\n",
        "\n",
        "  print(\"TP,FP,TN,FN: \",pTP[0],pFP[0],pTN[0],pFN[0])\n",
        "  pairAccuracies = [(tp + tn) / (tp + fp + tn + fn) for tp,fp,tn,fn in zip(pTP,pFP,pTN,pFN)]\n",
        "  return pairAccuracies\n",
        "\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,X,Y,train: bool):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.train = train\n",
        "    self.Random90DegFlip = Random90DegFlip(0.5)\n",
        "    self.RandomVerticalFlip = RandomVerticalFlip(0.5)\n",
        "    self.RandomHorizontalFlip = RandomHorizontalFlip(0.5)\n",
        "    self.gausNoise = v2.GaussianNoise()\n",
        "    self.erase = v2.RandomErasing(0.5,(0.01,0.2))\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    X,Yr = self.X[idx], self.Y[idx]\n",
        "    if self.train:\n",
        "      X,Yr = self.Random90DegFlip(X,Yr)\n",
        "      X,Yr = self.RandomVerticalFlip(X,Yr)\n",
        "      X,Yr = self.RandomHorizontalFlip(X,Yr)\n",
        "    X = T.functional.to_tensor(X)\n",
        "\n",
        "    if self.train:\n",
        "      X = self.gausNoise(X)\n",
        "      X = self.erase(X)\n",
        "    Yc = listToNum(Yr)\n",
        "    Yr,Yc = torch.tensor(Yr), torch.tensor(Yc)\n",
        "    return X,Yc,Yr\n",
        "\n",
        "\n",
        "def tensorToPil(image: Tensor) -> PIL.Image.Image:\n",
        "  img = T.functional.to_pil_image(image)\n",
        "  return img"
      ],
      "metadata": {
        "id": "OPPECqlZ47Ah"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n"
      ],
      "metadata": {
        "id": "oUrdPX7XrHtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch import Tensor\n",
        "from typing import cast\n",
        "from torchvision.transforms import v2\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "import PIL\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "torch.manual_seed(1)\n",
        "\n",
        "directory_in_str = \"data\"\n",
        "\n",
        "\n",
        "pathlist = Path(directory_in_str).glob('**/*.png')\n",
        "X = [0] * 10000\n",
        "\n",
        "for path in pathlist: # let's extract each of the 10000 images into one big tensor, our sample dataset, X\n",
        "    path_in_str = str(path)\n",
        "    index = path_in_str[-9:-4]\n",
        "\n",
        "    index = int(index)\n",
        "\n",
        "    img = Image.open(path_in_str).convert(\"L\")\n",
        "    X[index] = img # as such, image 00000 resides in X[0]\n",
        "\n",
        "\n",
        "\n",
        "csvFile = pd.read_csv(\"data/labels.csv\")\n",
        "Y = csvFile.values[:,1:].astype(np.float32)\n",
        "\n",
        "XTrain = X[:8000] # these are of the form, 8000 images\n",
        "YTrain = Y[:8000]\n",
        "\n",
        "\n",
        "XVal = X[8000:]\n",
        "YVal = Y[8000:]\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "Code to plot the first mentioned graph in the pdf\n",
        "numClasses = 135\n",
        "classCounts = np.zeros(numClasses)\n",
        "\n",
        "for y in YTrain:\n",
        "    idx = listToNum(y)\n",
        "    classCounts[idx] += 1\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(range(numClasses), classCounts)\n",
        "plt.xlabel('Class index')\n",
        "plt.ylabel('Number of samples')\n",
        "plt.title('Class distribution')\n",
        "plt.show()\n",
        "'''\n",
        "'''\n",
        "code to plot the second\n",
        "pairCounts = np.zeros(15)\n",
        "\n",
        "for y in YTrain:\n",
        "    pairIdx = numToPair(listToNum(y))\n",
        "    pairCounts[pairIdx] += 1\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(range(15), pairCounts)\n",
        "plt.xticks(range(15), [pairDict[i] for i in range(15)], rotation=45)\n",
        "plt.ylabel('Number of samples')\n",
        "plt.title('Unordered pair saturation')\n",
        "plt.show()\n",
        "'''\n",
        "trainDatasetAll = MyDataset(XTrain,YTrain,train=True)\n",
        "valDatasetAll = MyDataset(XVal,YVal,train=False)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "batchSize = 64\n",
        "testBatchSize = 2000\n",
        "\n",
        "train_loaderAll = torch.utils.data.DataLoader(trainDatasetAll, batch_size=batchSize, **kwargs)\n",
        "test_loaderAll = torch.utils.data.DataLoader(valDatasetAll, batch_size=testBatchSize, **kwargs)\n",
        "\n"
      ],
      "metadata": {
        "id": "Noooak7Y18Bx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Epoch\n"
      ],
      "metadata": {
        "id": "v_55-Uws4nQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Arch\n"
      ],
      "metadata": {
        "id": "cSgOW71w4qqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelCombined(torch.nn.Module):\n",
        "    def __init__(self, clsWeight: float = 0.7, regWeight: float = 0.3):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "        self.bb = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 8, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(8, 16, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(16, 32, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(32, 64, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(start_dim=1),\n",
        "            torch.nn.Linear(64 * 28 * 28, 256), torch.nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "        self.classPart = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(256,135),\n",
        "            torch.nn.BatchNorm1d(135),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        self.regPart = torch.nn.Sequential(\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(256,256),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(256,128),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(128,6)\n",
        "        )\n",
        "        self.clsWeight = float(clsWeight)\n",
        "        self.regWeight = float(regWeight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feat = self.bb(x)\n",
        "        return self.classPart(feat), self.regPart(feat)\n"
      ],
      "metadata": {
        "id": "6crfR75e4r8F"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainCombined(model, device, trainLoader, optimizer, epoch, logInterval):\n",
        "    model.train()\n",
        "    lossSum = 0\n",
        "    for batchIdx, (data, targetC, targetR) in enumerate(trainLoader):\n",
        "        data = data.to(device)\n",
        "        targetC = targetC.to(device).long()\n",
        "        targetR = targetR.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outC, outR = model(data)\n",
        "\n",
        "        lossC = torch.nn.functional.nll_loss(outC, targetC)\n",
        "        lossR = torch.nn.functional.smooth_l1_loss(outR, targetR)\n",
        "\n",
        "        loss = model.clsWeight * lossC + model.regWeight * lossR\n",
        "        lossSum += loss.item() * data.size(0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batchIdx % logInterval == 0:\n",
        "            done = batchIdx * data.size(0)\n",
        "            total = len(trainLoader.dataset)\n",
        "            print(\n",
        "                f\"Train Epoch: {epoch}, [{done}/{total} images ({done / total:.0%})] \"\n",
        "                + f\"loss {loss.item():.6f} (cls {lossC.item():.6f}, reg {lossR.item():.6f})\"\n",
        "            )\n",
        "    lossSum /= len(trainLoader.dataset)\n",
        "    return lossSum"
      ],
      "metadata": {
        "id": "-Z9hrOla4oVN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for epoch"
      ],
      "metadata": {
        "id": "l63Fs1SI4ZOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testCombined(model, device, valLoader):\n",
        "    model.eval()\n",
        "    eps = 1e-8 # to prevent division by zero(inspiration from Marek Muchas lecture about Batch Norm)\n",
        "    numClasses = 135\n",
        "\n",
        "\n",
        "    totalSamples = 0\n",
        "    lossSum = 0.0\n",
        "    correctTop1 = 0\n",
        "\n",
        "    # classification\n",
        "    TP = [0] * numClasses\n",
        "    FP = [0] * numClasses\n",
        "    FN = [0] * numClasses\n",
        "\n",
        "    # regression\n",
        "    sumAbs = 0\n",
        "    sumSq = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targetC, targetR in valLoader:\n",
        "            data = data.to(device)\n",
        "            targetC = targetC.to(device).long()\n",
        "            targetR = targetR.to(device).float()\n",
        "\n",
        "            batchSize = targetC.size(0)\n",
        "            totalSamples += batchSize\n",
        "\n",
        "            outC, outR = model(data)\n",
        "\n",
        "            lossC = torch.nn.functional.nll_loss(outC, targetC, reduction=\"mean\")\n",
        "            lossR = torch.nn.functional.smooth_l1_loss(outR, targetR, reduction=\"mean\")\n",
        "            jointLoss = model.clsWeight * lossC + model.regWeight * lossR\n",
        "            lossSum += jointLoss.item() * batchSize\n",
        "\n",
        "            preds = outC.argmax(dim=1)\n",
        "            correctTop1 += (preds == targetC).sum().item()\n",
        "\n",
        "            # accumulate TP/FP/FN per class\n",
        "            for c in range(numClasses):\n",
        "                predC = (preds == c)\n",
        "                trueC = (targetC == c)\n",
        "                tp = (predC & trueC).sum().item()\n",
        "                fp = (predC & ~trueC).sum().item()\n",
        "                fn = (~predC & trueC).sum().item()\n",
        "                TP[c] += tp\n",
        "                FP[c] += fp\n",
        "                FN[c] += fn\n",
        "\n",
        "            absErr = torch.abs(outR - targetR)\n",
        "            sqErr = (outR - targetR) ** 2\n",
        "            if sumAbs is None:\n",
        "                sumAbs = absErr.sum(dim=0).cpu()\n",
        "                sumSq = sqErr.sum(dim=0).cpu()\n",
        "            else:\n",
        "                sumAbs += absErr.sum(dim=0).cpu()\n",
        "                sumSq += sqErr.sum(dim=0).cpu()\n",
        "\n",
        "    avgLoss = lossSum / (totalSamples + eps)\n",
        "    top1 = correctTop1 / (totalSamples + eps)\n",
        "\n",
        "    precisionList = [TP[i] / (TP[i] + FP[i] + eps) for i in range(numClasses)]\n",
        "    recallList = [TP[i] / (TP[i] + FN[i] + eps) for i in range(numClasses)]\n",
        "    f1PerClass = [2 * precisionList[i] * recallList[i] / (precisionList[i] + recallList[i] + eps) for i in range(numClasses)]\n",
        "    macroF1 = sum(f1PerClass) / numClasses\n",
        "\n",
        "    pairTP = [0] * 15\n",
        "    pairFN = [0] * 15\n",
        "    for i in range(numClasses):\n",
        "        pidx = numToPair(i)\n",
        "        pairTP[pidx] += TP[i]\n",
        "        pairFN[pidx] += FN[i]\n",
        "    recallPerPair = [pairTP[i] / (pairTP[i] + pairFN[i] + eps) for i in range(15)]\n",
        "\n",
        "    sumAbs = sumAbs.float()\n",
        "    sumSq = sumSq.float()\n",
        "    maePerClass = (sumAbs / (totalSamples + eps)).tolist()\n",
        "    rmsePerClass = torch.sqrt(sumSq / (totalSamples + eps)).tolist()\n",
        "    maeOverall = float(sum(maePerClass) / len(maePerClass))\n",
        "    rmseOverall = float(torch.sqrt((sumSq / (totalSamples + eps)).mean()).item())\n",
        "\n",
        "    print(\"-----------------Validation-----------------\")\n",
        "    print(f\"Avg joint loss: {avgLoss:.6f}\")\n",
        "    print(f\"Top-1 accuracy: {top1:.4f}\")\n",
        "    print(f\"Macro F1: {macroF1:.4f}\")\n",
        "    print(\"Per-pair recall:\")\n",
        "    for i in range(15):\n",
        "        print(f\"  {pairDict[i]:15s}: {recallPerPair[i]:.4f}\")\n",
        "\n",
        "    print(f\"Regression MAE per class: {maePerClass}\")\n",
        "    print(f\"Regression RMSE per class: {rmsePerClass}\")\n",
        "    print(f\"Overall MAE: {maeOverall:.4f}, Overall RMSE: {rmseOverall:.4f}\")\n",
        "    print(\"---------------------------------------------\")\n",
        "\n",
        "    return {\n",
        "        \"loss\": avgLoss,\n",
        "        \"top1\": top1,\n",
        "        \"macroF1\": macroF1,\n",
        "        \"recallPerPair\": recallPerPair,\n",
        "        \"maePerClass\": maePerClass,\n",
        "        \"rmseOverall\": rmseOverall,\n",
        "        \"rmsePerClass\": rmsePerClass,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Pv4BhIhp4bUs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification Model\n"
      ],
      "metadata": {
        "id": "V0k-1q0-4JOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------------------- CLASS ONLY ----------------------------------------------------------------------\n",
        "model = ModelCombined(1,0).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "trainLosses = []\n",
        "valLosses = []\n",
        "valTop1s = []\n",
        "valRMSEs = []\n",
        "\n",
        "# let's initialise the weights using glorot in pytorch\n",
        "for m in model.classPart.modules():\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "for m in model.regPart.modules():\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    trainLoss = trainCombined(model, device, train_loaderAll, optimizer, epoch, logInterval=100)\n",
        "    metrics = testCombined(model, device, test_loaderAll)\n",
        "\n",
        "    trainLosses.append(trainLoss)\n",
        "    valLosses.append(metrics[\"loss\"])\n",
        "    valTop1s.append(metrics[\"top1\"])\n",
        "    valRMSEs.append(metrics[\"rmseOverall\"])\n",
        "\n",
        "\n",
        "# so now, trainLosses, valLosses, valTop1s, and valRMSEs, are in their respective lists.\n",
        "epochs = list(range(100))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, trainLosses, label=\"Train Loss\")\n",
        "plt.plot(epochs, valLosses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train/Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, valTop1s, label=\"Validation Top-1 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Top-1 Accuracy\")\n",
        "plt.title(\"Validation Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, valRMSEs, label=\"Validation RMSE (overall)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"Validation RMSE over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "metricsDf = pd.DataFrame({\n",
        "\"TrainLoss\": trainLosses,\n",
        "\"ValLoss\": valLosses,\n",
        "\"ValTop1Acc\": valTop1s,\n",
        "\"ValRMSE\": valRMSEs\n",
        "})\n",
        "\n",
        "print(metricsDf)\n",
        "metricsDf.to_csv(\"training_metrics.csv\", index=False)\n",
        "cOnlyTLoss = trainLosses[-1]\n",
        "cOnlyVLoss = valLosses[-1]\n",
        "cOnlyRMSE = valRMSEs[-1]\n",
        "cOnlyT1 = valTop1s[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lVJSdRoO4LJt",
        "outputId": "294a2b5e-afdc-4788-ea41-3b3f37dfa0ee"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0, [0/8000 images (0%)] loss 5.016137 (cls 5.016137, reg 2.077105)\n",
            "Train Epoch: 0, [6400/8000 images (80%)] loss 5.111771 (cls 5.111771, reg 2.005621)\n",
            "-----------------Validation-----------------\n",
            "Avg joint loss: 4.874215\n",
            "Top-1 accuracy: 0.0085\n",
            "Macro F1: 0.0005\n",
            "Per-pair recall:\n",
            "  square circle  : 0.0000\n",
            "  square up      : 0.0000\n",
            "  square right   : 0.0000\n",
            "  square down    : 0.0000\n",
            "  square left    : 0.0345\n",
            "  circle up      : 0.0000\n",
            "  circle right   : 0.0600\n",
            "  circle down    : 0.0000\n",
            "  circle left    : 0.0153\n",
            "  up right       : 0.0000\n",
            "  up down        : 0.0000\n",
            "  up left        : 0.0089\n",
            "  right down     : 0.0000\n",
            "  right left     : 0.0000\n",
            "  down left      : 0.0000\n",
            "Regression MAE per class: [1.7711650133132935, 1.8189409971237183, 1.770508050918579, 2.1621341705322266, 1.9689371585845947, 2.1702427864074707]\n",
            "Regression RMSE per class: [2.941011905670166, 3.175001621246338, 2.8999364376068115, 3.428433656692505, 3.321768283843994, 3.3711678981781006]\n",
            "Overall MAE: 1.9437, Overall RMSE: 3.1962\n",
            "---------------------------------------------\n",
            "Train Epoch: 1, [0/8000 images (0%)] loss 4.933342 (cls 4.933342, reg 1.947523)\n",
            "Train Epoch: 1, [6400/8000 images (80%)] loss 4.990443 (cls 4.990443, reg 1.980452)\n",
            "-----------------Validation-----------------\n",
            "Avg joint loss: 4.900843\n",
            "Top-1 accuracy: 0.0100\n",
            "Macro F1: 0.0019\n",
            "Per-pair recall:\n",
            "  square circle  : 0.0080\n",
            "  square up      : 0.0075\n",
            "  square right   : 0.0000\n",
            "  square down    : 0.0000\n",
            "  square left    : 0.0000\n",
            "  circle up      : 0.0935\n",
            "  circle right   : 0.0267\n",
            "  circle down    : 0.0000\n",
            "  circle left    : 0.0076\n",
            "  up right       : 0.0000\n",
            "  up down        : 0.0000\n",
            "  up left        : 0.0000\n",
            "  right down     : 0.0000\n",
            "  right left     : 0.0000\n",
            "  down left      : 0.0000\n",
            "Regression MAE per class: [1.9703359603881836, 1.894481897354126, 1.9043108224868774, 2.3505046367645264, 1.9945566654205322, 1.9132152795791626]\n",
            "Regression RMSE per class: [2.564396858215332, 3.1934943199157715, 3.18284273147583, 3.523858070373535, 3.1328110694885254, 3.1368844509124756]\n",
            "Overall MAE: 2.0046, Overall RMSE: 3.1352\n",
            "---------------------------------------------\n",
            "Train Epoch: 2, [0/8000 images (0%)] loss 4.832487 (cls 4.832487, reg 2.082346)\n",
            "Train Epoch: 2, [6400/8000 images (80%)] loss 4.786684 (cls 4.786684, reg 1.925371)\n",
            "-----------------Validation-----------------\n",
            "Avg joint loss: 4.772674\n",
            "Top-1 accuracy: 0.0180\n",
            "Macro F1: 0.0056\n",
            "Per-pair recall:\n",
            "  square circle  : 0.0480\n",
            "  square up      : 0.0000\n",
            "  square right   : 0.0248\n",
            "  square down    : 0.0227\n",
            "  square left    : 0.0138\n",
            "  circle up      : 0.0072\n",
            "  circle right   : 0.0200\n",
            "  circle down    : 0.0000\n",
            "  circle left    : 0.0000\n",
            "  up right       : 0.0000\n",
            "  up down        : 0.0000\n",
            "  up left        : 0.1607\n",
            "  right down     : 0.0000\n",
            "  right left     : 0.0000\n",
            "  down left      : 0.0000\n",
            "Regression MAE per class: [1.6727275848388672, 1.9254652261734009, 1.8353132009506226, 2.1086528301239014, 2.0317866802215576, 1.8711843490600586]\n",
            "Regression RMSE per class: [2.8589885234832764, 3.2410364151000977, 3.1462833881378174, 3.3892152309417725, 3.320012092590332, 3.1217288970947266]\n",
            "Overall MAE: 1.9075, Overall RMSE: 3.1841\n",
            "---------------------------------------------\n",
            "Train Epoch: 3, [0/8000 images (0%)] loss 4.851573 (cls 4.851573, reg 1.909659)\n",
            "Train Epoch: 3, [6400/8000 images (80%)] loss 4.707514 (cls 4.707514, reg 1.853607)\n",
            "-----------------Validation-----------------\n",
            "Avg joint loss: 4.720864\n",
            "Top-1 accuracy: 0.0200\n",
            "Macro F1: 0.0071\n",
            "Per-pair recall:\n",
            "  square circle  : 0.0240\n",
            "  square up      : 0.0075\n",
            "  square right   : 0.0083\n",
            "  square down    : 0.0227\n",
            "  square left    : 0.0069\n",
            "  circle up      : 0.0144\n",
            "  circle right   : 0.0200\n",
            "  circle down    : 0.0145\n",
            "  circle left    : 0.0153\n",
            "  up right       : 0.0000\n",
            "  up down        : 0.0000\n",
            "  up left        : 0.1518\n",
            "  right down     : 0.0410\n",
            "  right left     : 0.0000\n",
            "  down left      : 0.0000\n",
            "Regression MAE per class: [1.6670072078704834, 1.962893009185791, 1.8916332721710205, 2.125624418258667, 2.0285158157348633, 1.8310658931732178]\n",
            "Regression RMSE per class: [2.8652384281158447, 3.2849810123443604, 3.187512159347534, 3.394641637802124, 3.299065351486206, 3.095632791519165]\n",
            "Overall MAE: 1.9178, Overall RMSE: 3.1925\n",
            "---------------------------------------------\n",
            "Train Epoch: 4, [0/8000 images (0%)] loss 4.743917 (cls 4.743917, reg 1.861799)\n",
            "Train Epoch: 4, [6400/8000 images (80%)] loss 4.733511 (cls 4.733511, reg 1.924672)\n",
            "-----------------Validation-----------------\n",
            "Avg joint loss: 4.663134\n",
            "Top-1 accuracy: 0.0230\n",
            "Macro F1: 0.0078\n",
            "Per-pair recall:\n",
            "  square circle  : 0.0320\n",
            "  square up      : 0.0075\n",
            "  square right   : 0.0165\n",
            "  square down    : 0.0152\n",
            "  square left    : 0.0276\n",
            "  circle up      : 0.0144\n",
            "  circle right   : 0.0067\n",
            "  circle down    : 0.0290\n",
            "  circle left    : 0.0229\n",
            "  up right       : 0.0000\n",
            "  up down        : 0.0000\n",
            "  up left        : 0.1964\n",
            "  right down     : 0.0082\n",
            "  right left     : 0.0000\n",
            "  down left      : 0.0000\n",
            "Regression MAE per class: [1.6401419639587402, 1.9593908786773682, 1.8705406188964844, 2.1618452072143555, 2.069589853286743, 1.893619418144226]\n",
            "Regression RMSE per class: [2.8011274337768555, 3.254608631134033, 3.1369471549987793, 3.4213640689849854, 3.313730001449585, 3.132983922958374]\n",
            "Overall MAE: 1.9325, Overall RMSE: 3.1828\n",
            "---------------------------------------------\n",
            "Train Epoch: 5, [0/8000 images (0%)] loss 4.627327 (cls 4.627327, reg 1.874922)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1362625651.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrainLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainCombined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loaderAll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogInterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestCombined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loaderAll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1758744893.py\u001b[0m in \u001b[0;36mtrainCombined\u001b[0;34m(model, device, trainLoader, optimizer, epoch, logInterval)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlossSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatchIdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtargetC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression Model\n"
      ],
      "metadata": {
        "id": "Ri5f0-P94C8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------- REGRESSION ONLY ----------------------------------------------------------------------\n",
        "model = ModelCombined(0,1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "trainLosses = []\n",
        "valLosses = []\n",
        "valTop1s = []\n",
        "valRMSEs = []\n",
        "\n",
        "for m in model.classPart.modules():\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "for m in model.regPart.modules():\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    trainLoss = trainCombined(model, device, train_loaderAll, optimizer, epoch, logInterval=100)\n",
        "    metrics = testCombined(model, device, test_loaderAll)\n",
        "\n",
        "    trainLosses.append(trainLoss)\n",
        "    valLosses.append(metrics[\"loss\"])\n",
        "    valTop1s.append(metrics[\"top1\"])\n",
        "    valRMSEs.append(metrics[\"rmseOverall\"])\n",
        "\n",
        "\n",
        "epochs = list(range(100))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, trainLosses, label=\"Train Loss\")\n",
        "plt.plot(epochs, valLosses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train/Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, valTop1s, label=\"Validation Top-1 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Top-1 Accuracy\")\n",
        "plt.title(\"Validation Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, valRMSEs, label=\"Validation RMSE (overall)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"Validation RMSE over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "metricsDf = pd.DataFrame({\n",
        "\"TrainLoss\": trainLosses,\n",
        "\"ValLoss\": valLosses,\n",
        "\"ValTop1Acc\": valTop1s,\n",
        "\"ValRMSE\": valRMSEs\n",
        "})\n",
        "\n",
        "rOnlyTLoss = trainLosses[-1]\n",
        "rOnlyVLoss = valLosses[-1]\n",
        "rOnlyRMSE = valRMSEs[-1]\n",
        "rOnlyT1 = valTop1s[-1]"
      ],
      "metadata": {
        "id": "0ekt6xIE4E3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Model"
      ],
      "metadata": {
        "id": "8IsZZ6OI31yS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------- COMBINED ----------------------------------------------------------------------------\n",
        "# last model, with combined classification and regression\n",
        "model = ModelCombined().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "trainLosses = []\n",
        "valLosses = []\n",
        "valTop1s = []\n",
        "valRMSEs = []\n",
        "\n",
        "for m in model.classPart.modules():\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "for m in model.regPart.modules():\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "\n",
        "for epoch in range(100):\n",
        "    trainLoss = trainCombined(model, device, train_loaderAll, optimizer, epoch, logInterval=100)\n",
        "    metrics = testCombined(model, device, test_loaderAll)\n",
        "\n",
        "    trainLosses.append(trainLoss)\n",
        "    valLosses.append(metrics[\"loss\"])\n",
        "    valTop1s.append(metrics[\"top1\"])\n",
        "    valRMSEs.append(metrics[\"rmseOverall\"])\n",
        "\n",
        "\n",
        "epochs = list(range(100))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, trainLosses, label=\"Train Loss\")\n",
        "plt.plot(epochs, valLosses, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train/Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, valTop1s, label=\"Validation Top-1 Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Top-1 Accuracy\")\n",
        "plt.title(\"Validation Accuracy over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, valRMSEs, label=\"Validation RMSE (overall)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.title(\"Validation RMSE over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "metricsDf = pd.DataFrame({\n",
        "\"TrainLoss\": trainLosses,\n",
        "\"ValLoss\": valLosses,\n",
        "\"ValTop1Acc\": valTop1s,\n",
        "\"ValRMSE\": valRMSEs\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "bothTLoss = trainLosses[-1]\n",
        "bothVLoss = valLosses[-1]\n",
        "bothRMSE = valRMSEs[-1]\n",
        "bothT1 = valTop1s[-1]\n",
        "\n",
        "finalMetrics = pd.DataFrame({\n",
        "\"Model\": [\"Class Only\", \"Regression Only\", \"Combined\"],\n",
        "\"TrainLoss\": [cOnlyTLoss, rOnlyTLoss, bothTLoss],\n",
        "\"ValLoss\": [cOnlyVLoss, rOnlyVLoss, bothVLoss],\n",
        "\"RMSE\": [cOnlyRMSE, rOnlyRMSE, bothRMSE],\n",
        "\"Top1\": [cOnlyT1, rOnlyT1, bothT1]\n",
        "\n",
        "})\n",
        "finalMetrics.to_csv(\"final_metrics.csv\", index=False)\n",
        "print(finalMetrics)"
      ],
      "metadata": {
        "id": "FeXRrsXn9T3j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
