{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: Multitask Learning for Geometric Shape Classification and Counting**\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "In this project, you will design, implement, and evaluate a **multitask neural network** that performs **two tasks simultaneously**:\n",
        "\n",
        "1. **Classification** – identify which pair of geometric shape types appears in a 28×28 binary image (135 possible configurations).\n",
        "2. **Regression** – predict how many shapes of each type are present (6 regression targets).\n",
        "\n",
        "This project focuses on **multi-task learning**, i.e., using one shared model to learn several related tasks at once. You will compare how adding an auxiliary task affects performance and training dynamics.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dataset\n",
        "\n",
        "You will use the **Geometric Shape Numbers (GSN)** dataset:\n",
        "\n",
        "```bash\n",
        "!wget https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip\n",
        "```\n",
        "\n",
        "This will create a directory `data/` containing:\n",
        "\n",
        "* **10,000 images** (28×28x1, grayscale)\n",
        "* **labels.csv** – counts of each of six shape types per image\n",
        "\n",
        "Each image contains exactly **two types** of geometric figures (out of six) and **10 shapes total**.\n",
        "\n",
        "**Shape classes:**\n",
        "\n",
        "| Index | Shape type     |\n",
        "| ----: | -------------- |\n",
        "|     0 | square         |\n",
        "|     1 | circle         |\n",
        "|     2 | triangle up    |\n",
        "|     3 | triangle right |\n",
        "|     4 | triangle down  |\n",
        "|     5 | triangle left  |\n",
        "\n",
        "Example row from `labels.csv`:\n",
        "\n",
        "```\n",
        "name,squares,circles,up,right,down,left\n",
        "img_00000.png,0,0,0,4,0,6\n",
        "```\n",
        "\n",
        "Here, the image contains **4 right-pointing triangles** and **6 left-pointing triangles**.\n",
        "\n",
        "**Split:**\n",
        "\n",
        "* Training: first 9,000 samples\n",
        "* Validation: last 1,000 samples\n",
        "\n",
        "Examples:\n",
        "![example.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Tasks and objectives\n",
        "\n",
        "You must design a **multitask deep learning system** that:\n",
        "\n",
        "1. **Classifies** each image into one of **135 possible configurations**, representing:\n",
        "\n",
        "   * which **two shape classes** appear, and\n",
        "   * how their counts (1–9) sum to 10.\n",
        "\n",
        "   → Example: \"3 circles + 7 squares\" is one configuration class.\n",
        "\n",
        "2. **Regresses** the number of shapes of each type (a 6-dimensional real-valued output).\n",
        "\n",
        "3. Combines both objectives in a **joint loss** function (Hint: losses are implemented in PyTorch):\n",
        "\n",
        "\n",
        "$$ Loss = \\text{NLLLoss(classification)} + \\lambda_{\\text{cnt}} \\cdot \\text{SmoothL1Loss(regression)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Model requirements\n",
        "\n",
        "### Architecture constraints\n",
        "\n",
        "You must use **exactly this feature extractor (backbone)**:\n",
        "\n",
        "```python\n",
        "nn.Sequential(\n",
        "    nn.Conv2d(1, 8, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(8, 16, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(16, 32, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Conv2d(32, 64, 3, stride=1, padding=1), nn.ReLU(),\n",
        "    nn.Flatten(start_dim=1),\n",
        "    nn.Linear(64 * 28 * 28, 256), nn.ReLU()\n",
        ")\n",
        "```\n",
        "\n",
        "Then add **two separate heads**:\n",
        "\n",
        "* `head_cls`: outputs log-probabilities for 135 classes\n",
        "* `head_cnt`: outputs 6 regression values (counts)\n",
        "\n",
        "The model must return two outputs: `(log_probs, counts)`.\n",
        "\n",
        "You may add dropout or batch normalization inside the heads, **but you must not modify the backbone**.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Training setup\n",
        "\n",
        "* Optimizer: **Adam**, learning rate = 1e-3\n",
        "* Epochs: up to **100** (use **early stopping**)\n",
        "* Batch sizes: **64** (train), **1000** (validation)\n",
        "* Device: GPU allowed for Notebook, but your **final code must run on GPU within ~30 minutes**\n",
        "* Random seed: set `torch.manual_seed(1)` for reproducibility\n",
        "* Split: **exactly 9,000 train / 1,000 validation**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Data preprocessing and augmentation\n",
        "\n",
        "You must implement a **PyTorch `Dataset` class** that:\n",
        "\n",
        "* Reads `labels.csv`\n",
        "* Loads the corresponding image (from `data/`)\n",
        "* Returns both:\n",
        "  * the image (as a tensor)\n",
        "  * the labels (counts for 6 shapes)\n",
        "* Optionally applies transformations\n",
        "\n",
        "### Required augmentations\n",
        "\n",
        "You must implement **at least three** of the following:\n",
        "\n",
        "1. Random horizontal flip\n",
        "2. Random vertical flip\n",
        "3. Random 90° rotation (must correctly rotate orientation labels: up → right → down → left)\n",
        "4. Random brightness/contrast (mild)\n",
        "5. Gaussian noise\n",
        "6. Random erasing (small areas only)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Evaluation metrics\n",
        "\n",
        "Implement and report the following metrics on the validation set:\n",
        "\n",
        "### (a) **Classification (135-way)**\n",
        "\n",
        "* Top-1 accuracy\n",
        "* Macro F1-score\n",
        "* Per-pair accuracy (aggregate by unordered shape pair, e.g. {circle, up})\n",
        "\n",
        "### (b) **Regression (6-D counts)**\n",
        "\n",
        "* RMSE per class and overall\n",
        "* MAE per class and overall\n",
        "\n",
        "Also plot:\n",
        "\n",
        "* Training and validation losses\n",
        "* Validation accuracy and RMSE over epochs\n",
        "\n",
        "**Important**: This task is not about finding the best architecture; we expect at least 50% accuracy, but achieving results higher than that will not affect the grade for the assignment**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Experiments and analysis\n",
        "\n",
        "You must train and compare **three model settings**:\n",
        "\n",
        "| Setting | Description                                      |\n",
        "| :------ | :----------------------------------------------- |\n",
        "| 1       | **Classification-only:** λ_cnt = 0               |\n",
        "| 2       | **Regression-only:** classification loss ignored |\n",
        "| 3       | **Multitask:** λ_cnt = with your choose          |\n",
        "\n",
        "For each experiment:\n",
        "\n",
        "* Train until early stopping\n",
        "* Record loss, accuracy, RMSE, and runtime\n",
        "* Compare results and explain how λ influences learning\n",
        "* Discuss whether multitask learning improves the main tasks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Final deliverables\n",
        "\n",
        "You must submit .zip project with:\n",
        "\n",
        "1. **Code** (`.ipynb` or `.py`) that:\n",
        "\n",
        "   * Downloads and extracts the dataset\n",
        "   * Defines dataset, dataloaders, model, loss, training loop, evaluation, and plotting\n",
        "   * Can run start-to-end without interaction, and finishes within 30 minutes on Colab T4 GPUs\n",
        "   * Includes three experiment configurations\n",
        "\n",
        "2. **Report (2–4 pages, PDF)** including:\n",
        "   * Section on (EDA) Exploratory Data Analysis in your report: no more than 3 graphs or tables describing the data set.\n",
        "   * Model architecture\n",
        "   * Description and justification of augmentations\n",
        "   * Results table (loss, accuracy, RMSE for all runs)\n",
        "   * Learning curves\n",
        "   * Discussion on multitask effects\n",
        "\n",
        "3. **README.md**:\n",
        "\n",
        "   * Link to Colab version of task for fast replication.\n",
        "   * Approximate runtime and resource requirements\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Grading rubric\n",
        "\n",
        "Component\tDescription\tPoints\n",
        "1. Implementation correctness\tCorrect use of the fixed backbone, two-headed model, and proper training loop (classification + regression).\t30%\n",
        "2. Data & augmentations\tProper dataset loading, preprocessing, and at least three augmentations with brief justification.\t20%\n",
        "3. Evaluation & experiments\tCorrect computation of metrics (accuracy, F1, RMSE) and completion of all three λ configurations (λ=0, regression-only, your choice λ).\t30%\n",
        "4. Report & analysis\n",
        "A clear separation of concerns (e.g. headers in notebooks, modules in code) and concise 2–4 page report with results tables, learning curves, confusion matrix, and short discussion on multitask effects and error examples.\n",
        "20%\n",
        "\n",
        "###### Readability and modularity will be considered within each grading component. Clear structure (headers in notebooks, docstrings, modular code) significantly improves evaluation speed. Emphasize using clear headers to help reviewers navigate efficiently.\n",
        "---"
      ],
      "metadata": {
        "id": "_NvRrg8YvTPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
        "!unzip data_gsn.zip &> /dev/null\n",
        "!rm data_gsn.zip\n",
        "\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch import Tensor\n",
        "from typing import cast\n",
        "from torchvision.transforms import v2\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "import PIL\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUmggC-uvcGR",
        "outputId": "625dfddf-f96d-407a-95a8-77a7383a884a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-24 23:07:39--  https://github.com/marcin119a/data/raw/refs/heads/main/data_gsn.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip [following]\n",
            "--2025-11-24 23:07:40--  https://raw.githubusercontent.com/marcin119a/data/refs/heads/main/data_gsn.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5544261 (5.3M) [application/zip]\n",
            "Saving to: ‘data_gsn.zip’\n",
            "\n",
            "data_gsn.zip        100%[===================>]   5.29M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-24 23:07:40 (102 MB/s) - ‘data_gsn.zip’ saved [5544261/5544261]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "torch.manual_seed(1)\n",
        "\n",
        "directory_in_str = \"data\"\n",
        "class RandomHorizontalFlip(torch.nn.Module): # here define it for both the img and the label, which itsel is just a list\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, label: np.ndarray):\n",
        "        imgNew = img.copy()\n",
        "        labelNew = label.copy()\n",
        "        if torch.rand(1) < self.p:\n",
        "            imgNew =  T.functional.hflip(imgNew)\n",
        "            placeHold = labelNew[5]\n",
        "            labelNew[5] = labelNew[3]\n",
        "            labelNew[3] = placeHold\n",
        "\n",
        "        return imgNew, labelNew\n",
        "\n",
        "\n",
        "class RandomVerticalFlip(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, label: np.ndarray):\n",
        "        imgNew = img.copy()\n",
        "        labelNew = label.copy()\n",
        "        if torch.rand(1) < self.p:\n",
        "\n",
        "            imgNew =  T.functional.hflip(imgNew)\n",
        "            placeHold = labelNew[2]\n",
        "            labelNew[2] = labelNew[4]\n",
        "            labelNew[4] = placeHold\n",
        "\n",
        "        return imgNew, labelNew\n",
        "\n",
        "class Random90DegFlip(torch.nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, img, label: np.ndarray):\n",
        "        imgNew = img.copy()\n",
        "        labelNew = label.copy()\n",
        "        if torch.rand(1) < self.p:\n",
        "            imgNew =  T.functional.rotate(imgNew,90)\n",
        "            plcHoldUp = labelNew[2]\n",
        "            plcHolLeft = labelNew[5]\n",
        "            plcHolDown = labelNew[4]\n",
        "            plcHolRight = labelNew[3]\n",
        "\n",
        "            labelNew[2] = plcHolLeft # 2 becomes 5\n",
        "            labelNew[3] = plcHoldUp # 3 becomes 2\n",
        "            labelNew[4] = plcHolRight # 4 becomes 3\n",
        "            labelNew[5] = plcHolDown # 5 becomes 4\n",
        "        return imgNew, labelNew\n",
        "\n",
        "\n",
        "indexDict = {\n",
        "    \"01\":0,\n",
        "    \"02\":1,\n",
        "    \"03\":2,\n",
        "    \"04\":3,\n",
        "    \"05\":4,\n",
        "    \"12\":5,\n",
        "    \"13\":6,\n",
        "    \"14\":7,\n",
        "    \"15\":8,\n",
        "    \"23\":9,\n",
        "    \"24\":10,\n",
        "    \"25\":11,\n",
        "    \"34\":12,\n",
        "    \"35\":13,\n",
        "    \"45\":14\n",
        "}\n",
        "\n",
        "valDict = {\n",
        "    \"19\":0,\n",
        "    \"28\":1,\n",
        "    \"37\":2,\n",
        "    \"46\":3,\n",
        "    \"55\":4,\n",
        "    \"64\":5,\n",
        "    \"73\":6,\n",
        "    \"82\":7,\n",
        "    \"91\":8,\n",
        "}\n",
        "\n",
        "def listToNum(y):\n",
        "  # here y is [0,0,3,7,0,0]\n",
        "  val1,val2,i1,i2 = -1,-1,-1,-1\n",
        "  for i in range(6):\n",
        "    if y[i] != 0:\n",
        "      if val1 == -1:\n",
        "        val1 = y[i]\n",
        "        i1 = i\n",
        "      else:\n",
        "        val2 = y[i]\n",
        "        i2 = i\n",
        "  ixPair = str(int(i1)) + str(int(i2))\n",
        "  ixVal = indexDict[ixPair]\n",
        "\n",
        "  valPair = str(int(val1)) + str(int(val2))\n",
        "  valVal = valDict[valPair]\n",
        "  finalIx = ixVal * 9 + valVal\n",
        "  #print(\"Y: \",y)\n",
        "  #print(\"ixPair and valPair: \",ixVal,valVal)\n",
        "  #print(\"final index: \", finalIx)\n",
        "\n",
        "  return finalIx\n",
        "\n",
        "\n",
        "\n",
        "class MyDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.Random90DegFlip = Random90DegFlip()\n",
        "    self.RandomVerticalFlip = RandomVerticalFlip()\n",
        "    self.RandomHorizontalFlip = RandomHorizontalFlip()\n",
        "    self.gausNoise = v2.GaussianNoise()\n",
        "    self.colorJit = v2.ColorJitter()\n",
        "    self.erase = v2.RandomErasing()\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    X,Yr = self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "    X,Yr = self.Random90DegFlip(X,Yr)\n",
        "    X,Yr = self.RandomVerticalFlip(X,Yr)\n",
        "    X,Yr = self.RandomHorizontalFlip(X,Yr)\n",
        "    X = T.functional.to_tensor(X)\n",
        "\n",
        "    X = self.gausNoise(X)\n",
        "    X = self.colorJit(X)\n",
        "    X = self.erase(X)\n",
        "    Yc = listToNum(Yr)\n",
        "    Yr,Yc = torch.tensor(Yr), torch.tensor(Yc)\n",
        "    return X,Yc,Yr\n",
        "\n",
        "\n",
        "def tensorToPil(image: Tensor) -> PIL.Image.Image:\n",
        "  img = T.functional.to_pil_image(image)\n",
        "  return img\n",
        "\n",
        "\n",
        "class MyDatasetRegression(torch.utils.data.Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.Random90DegFlip = Random90DegFlip()\n",
        "    self.RandomVerticalFlip = RandomVerticalFlip()\n",
        "    self.RandomHorizontalFlip = RandomHorizontalFlip()\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    X,Yr = self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "    X,Yr = self.Random90DegFlip(X,Yr)\n",
        "    X,Yr = self.RandomVerticalFlip(X,Yr)\n",
        "    X,Yr = self.RandomHorizontalFlip(X,Yr)\n",
        "    X = T.functional.to_tensor(X)\n",
        "\n",
        "    return X,Yr\n",
        "\n",
        "\n",
        "class MyDatasetClassification(torch.utils.data.Dataset):\n",
        "  def __init__(self,X,Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    self.Random90DegFlip = Random90DegFlip()\n",
        "    self.RandomVerticalFlip = RandomVerticalFlip()\n",
        "    self.RandomHorizontalFlip = RandomHorizontalFlip()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    X,Y = self.X[idx], self.Y[idx]\n",
        "    '''\n",
        "    X,Y = self.Random90DegFlip(X,Y)\n",
        "    X,Y = self.RandomVerticalFlip(X,Y)\n",
        "    X,Y = self.RandomHorizontalFlip(X,Y)\n",
        "    '''\n",
        "    X = T.functional.to_tensor(X)\n",
        "    Y = listToNum(Y)\n",
        "\n",
        "    return X,Y\n",
        "\n",
        "\n",
        "def tensorToPil(image: Tensor) -> PIL.Image.Image:\n",
        "  img = T.functional.to_pil_image(image)\n",
        "  return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ModelCombined(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bb = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(1, 8, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(8, 16, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(16, 32, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(32, 64, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "    torch.nn.Flatten(start_dim=1),\n",
        "    torch.nn.Linear(64 * 28 * 28, 256), torch.nn.ReLU()\n",
        ")\n",
        "    self.classPart = torch.nn.Sequential(\n",
        "        torch.nn.BatchNorm1d(256),\n",
        "        torch.nn.Linear(256,256),\n",
        "        torch.nn.Sigmoid(),\n",
        "        torch.nn.BatchNorm1d(256),\n",
        "        torch.nn.Linear(256,135),\n",
        "        torch.nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "    self.regPart = torch.nn.Sequential(\n",
        "        torch.nn.BatchNorm1d(256),\n",
        "        torch.nn.Linear(256,6),\n",
        "        torch.nn.ReLU()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.bb(x)\n",
        "    return self.classPart(x), self.regPart(x) # use the same head backbone, output two different things\n",
        "\n",
        "\n",
        "def train(model, device, trainLoader,opt, epoch,logInterv):\n",
        "  model.train()\n",
        "  for batch_idx, (data, targetC,targetR) in enumerate(trainLoader):\n",
        "    data, targetC,targetR = data.to(device), targetC.to(device),targetR.to(device)\n",
        "    opt.zero_grad()\n",
        "    outputC, outputR = model(data)\n",
        "    #print(\"dziala 1\")\n",
        "    #print(\"OUTPUT 2 and TARGET 2: \", outputC.shape, targetC.shape, outputR.shape,targetR.shape)\n",
        "    lossC = torch.nn.functional.nll_loss(outputC,targetC)\n",
        "    #print(\"dziala 2 \")\n",
        "    lossR = torch.nn.functional.smooth_l1_loss(outputR,targetR)\n",
        "\n",
        "\n",
        "    loss = 0.7 * lossC   +  0.3 * lossR # let's try linear loss at first\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "    if batch_idx % logInterv == 0:\n",
        "      done, total = batch_idx * len(data) , len(trainLoader.dataset)\n",
        "      print(\n",
        "          f\"Train Epoch: {epoch}, [{done}/{total} images ({done / total:.0%})] \\t\"\n",
        "          + f\"loss {loss.item():.6f}\" + f\" Partial Loss: {lossC.item():.6f}\" + f\" LossR: {lossR.item():.6f}\"\n",
        "      )\n",
        "\n",
        "\n",
        "def test(model, device, trainLoader, epoch, logInterv):\n",
        "  model.eval()\n",
        "  testLoss = 0\n",
        "  total = 0\n",
        "  correctR = 0\n",
        "\n",
        "  correctC = 0\n",
        "  with torch.no_grad():\n",
        "    for data, targetC, targetR in trainLoader:\n",
        "      data, targetC, targetR = data.to(device), targetC.to(device), targetR.to(device)\n",
        "      outputC,outputR = model(data)\n",
        "      #print(\"OUTPUT: \", outputR,\" TARGET: \", targetR)\n",
        "      loss = 0.5 * torch.nn.functional.nll_loss(outputC,targetC) + 0.5 * torch.nn.functional.smooth_l1_loss(outputR,targetR)\n",
        "      testLoss += loss\n",
        "\n",
        "      outputC = outputC.argmax(dim=1) # this outputs C numbers, so our predictions\n",
        "      #print(\"Target C: \", targetC, \" output c: \", outputC)\n",
        "\n",
        "      correctR += (outputR == targetR).sum().item()\n",
        "      total += trainLoader.batch_size\n",
        "\n",
        "      correctC += (outputC == targetC).sum().item()\n",
        "\n",
        "    testLoss /= total\n",
        "\n",
        "    print(f\"Test loss: {testLoss}, For classification: {correctC} / {total}, Regression: {correctR} / {total}\" )\n",
        "\n",
        "\n",
        "\n",
        "class ModelReg(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bb = torch.nn.Sequential(\n",
        "      torch.nn.Conv2d(1, 8, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Conv2d(8, 16, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Conv2d(16, 32, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Conv2d(32, 64, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Flatten(start_dim=1),\n",
        "      torch.nn.Linear(64 * 28 * 28, 256), torch.nn.ReLU()\n",
        ")\n",
        "    self.head = torch.nn.Sequential(\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.BatchNorm1d(256),\n",
        "        torch.nn.Sigmoid(),\n",
        "        torch.nn.Linear(256,6),\n",
        "        torch.nn.ReLU()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.bb(x)\n",
        "    x = self.head(x)\n",
        "    return x\n",
        "\n",
        "def trainReg(model, device, trainLoader,opt, epoch,logInterv):\n",
        "  model.train()\n",
        "  for batch_idx, (data, targetR) in enumerate(trainLoader):\n",
        "    data, targetR = data.to(device), targetR.to(device)\n",
        "    opt.zero_grad()\n",
        "    outputR = model(data)\n",
        "\n",
        "\n",
        "    #outputC = outputC.long()\n",
        "    targetR = targetR.long()\n",
        "    lossR = torch.nn.functional.smooth_l1_loss(outputR,targetR)\n",
        "\n",
        "\n",
        "    lossR.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "    if batch_idx % logInterv == 0:\n",
        "      done, total = batch_idx * len(data) , len(trainLoader.dataset)\n",
        "      print(\n",
        "          f\"Train Epoch: {epoch}, [{done}/{total} images ({done / total:.0%})] \\t\"\n",
        "          + f\" Partial Loss: {lossR.item():.6f}\"\n",
        "      )\n",
        "\n",
        "\n",
        "def testReg(model, device, trainLoader, epoch, logInterv):\n",
        "  model.eval()\n",
        "  testLoss = 0\n",
        "  total = 0\n",
        "  correctR = 0\n",
        "  totalRMSEClass = 0\n",
        "  totalRMSE = 0\n",
        "\n",
        "  totalMAE = 0\n",
        "  totalMAEClass = 0\n",
        "\n",
        "\n",
        "  correctR = 0\n",
        "  with torch.no_grad():\n",
        "    for data, targetR in trainLoader:\n",
        "\n",
        "      data, targetR = data.to(device), targetR.to(device)\n",
        "      outputR = model(data)\n",
        "      targetR = targetR.long()\n",
        "\n",
        "      loss = torch.nn.functional.smooth_l1_loss(outputR,targetR)\n",
        "      testLoss += loss\n",
        "\n",
        "      total += trainLoader.batch_size\n",
        "\n",
        "      totalMAE += torch.abs((outputR - targetR)).sum().item()\n",
        "      totalMAEClass += torch.abs((outputR - targetR))\n",
        "\n",
        "      totalRMSE += ((outputR - targetR)**2).sum().item()\n",
        "      totalRMSEClass += (outputR - targetR)**2\n",
        "\n",
        "    testLoss /= total\n",
        "\n",
        "    totalRMSE /= total\n",
        "    totalRMSEClass /= total\n",
        "\n",
        "    totalRMSE = totalRMSE**0.5\n",
        "    totalRMSEClass = totalRMSEClass**0.5\n",
        "    totalRMSEClass = torch.sum(totalRMSEClass,0)\n",
        "\n",
        "    totalMAE /= total\n",
        "    totalMAEClass /= total\n",
        "\n",
        "    totalMAEClass = torch.sum(totalMAEClass,0)\n",
        "\n",
        "\n",
        "    print(f\"Test loss: {testLoss}, \\n  total RMSE: {totalRMSE}, total MAE: {totalMAE}, \\n class RMSE: {totalRMSEClass.tolist()}, class MAE: {totalMAEClass.tolist()}\" )\n",
        "\n",
        "\n",
        "class ModelClass(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.bb = torch.nn.Sequential(\n",
        "      torch.nn.Conv2d(1, 8, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Conv2d(8, 16, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Conv2d(16, 32, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Conv2d(32, 64, 3, stride=1, padding=1), torch.nn.ReLU(),\n",
        "      torch.nn.Flatten(start_dim=1),\n",
        "      torch.nn.Linear(64 * 28 * 28, 256), torch.nn.ReLU()\n",
        ")\n",
        "    self.head = torch.nn.Sequential(\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.BatchNorm1d(256),\n",
        "        torch.nn.Sigmoid(),\n",
        "        torch.nn.BatchNorm1d(256),\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.Linear(256,135),\n",
        "        torch.nn.BatchNorm1d(135),\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.Sigmoid(),\n",
        "        torch.nn.BatchNorm1d(135),\n",
        "        torch.nn.Dropout(),\n",
        "        torch.nn.Linear(135,135),\n",
        "        torch.nn.LogSoftmax(dim=1)\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    x = self.bb(x)\n",
        "    x = self.head(x)\n",
        "    return x\n",
        "\n",
        "def trainClass(model, device, trainLoader,opt, epoch,logInterv):\n",
        "  model.train()\n",
        "  for batch_idx, (data, targetC) in enumerate(trainLoader):\n",
        "    data, targetC = data.to(device), targetC.to(device)\n",
        "    opt.zero_grad()\n",
        "    outputC = model(data)\n",
        "    #print(\"Given input: \", outputC.shape, \"and required output: \",targetC.shape)\n",
        "\n",
        "\n",
        "    #outputC = outputC.long()\n",
        "    targetC = targetC.long()\n",
        "    lossC = torch.nn.functional.nll_loss(outputC,targetC)\n",
        "\n",
        "\n",
        "    lossC.backward()\n",
        "\n",
        "    opt.step()\n",
        "\n",
        "\n",
        "    if batch_idx % logInterv == 0:\n",
        "      done, total = batch_idx * len(data) , len(trainLoader.dataset)\n",
        "      print(\n",
        "          f\"Train Epoch: {epoch}, [{done}/{total} images ({done / total:.0%})] \\t\"\n",
        "          + f\" Partial Loss: {lossC.item():.6f}\"\n",
        "      )\n",
        "\n",
        "\n",
        "def testClass(model, device, trainLoader, epoch, logInterv):\n",
        "  model.eval()\n",
        "  testLoss = 0\n",
        "  total = 0\n",
        "  correctR = 0\n",
        "\n",
        "  correctC = 0\n",
        "  with torch.no_grad():\n",
        "    for data, targetC in trainLoader:\n",
        "\n",
        "      data, targetC = data.to(device), targetC.to(device)\n",
        "      outputC = model(data)\n",
        "      targetC = targetC.long()\n",
        "\n",
        "      loss = torch.nn.functional.nll_loss(outputC,targetC)\n",
        "      testLoss += loss\n",
        "\n",
        "\n",
        "      outputC = outputC.argmax(dim=1)\n",
        "\n",
        "\n",
        "      total += trainLoader.batch_size\n",
        "\n",
        "      correctC += (outputC == targetC).sum().item()\n",
        "\n",
        "    testLoss /= total\n",
        "\n",
        "    print(f\"Test loss: {testLoss}, For classification: {correctC} / {total}\" )\n",
        "pathlist = Path(directory_in_str).glob('**/*.png')\n",
        "X = [0] * 10000\n",
        "\n",
        "for path in pathlist: # let's extract each of the 10000 images into one big tensor, our sample dataset, X\n",
        "    path_in_str = str(path)\n",
        "    index = path_in_str[-9:-4]\n",
        "\n",
        "    index = int(index)\n",
        "\n",
        "    img = Image.open(path_in_str).convert(\"L\")\n",
        "    X[index] = img # as such, image 00000 resides in X[0]\n",
        "\n",
        "\n",
        "\n",
        "csvFile = pd.read_csv(\"data/labels.csv\")\n",
        "Y = csvFile.values[:,1:].astype(np.float32)\n",
        "\n",
        "XTrainReg = X[:8000] # these are of the form, 8000 images\n",
        "YTrainReg = Y[:8000]\n",
        "\n",
        "\n",
        "XValReg = X[8000:]\n",
        "YValReg = Y[8000:]\n",
        "\n",
        "\n",
        "XTrainClass = XTrainReg.copy()\n",
        "YTrainClass = YTrainReg.copy()\n",
        "\n",
        "XValClass = XValReg.copy()\n",
        "YValClass = YValReg.copy()\n",
        "\n",
        "\n",
        "trainDatasetC = MyDatasetClassification(XTrainClass,YTrainClass)\n",
        "valDatasetC = MyDatasetClassification(XValClass,YValClass)\n",
        "\n",
        "trainDatasetR = MyDatasetRegression(XTrainReg,YTrainReg)\n",
        "valDatasetR = MyDatasetRegression(XValReg,YValReg)\n",
        "\n",
        "trainDatasetAll = MyDataset(XTrainReg,YTrainReg)\n",
        "valDatasetAll = MyDataset(XValReg,YValReg)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "batchSize = 64\n",
        "testBatchSize = 1000\n",
        "\n",
        "train_loaderC = torch.utils.data.DataLoader(trainDatasetC, batch_size=batchSize, **kwargs)\n",
        "test_loaderC = torch.utils.data.DataLoader(valDatasetC, batch_size=testBatchSize, **kwargs)\n",
        "\n",
        "train_loaderR = torch.utils.data.DataLoader(trainDatasetR, batch_size=batchSize, **kwargs)\n",
        "test_loaderR = torch.utils.data.DataLoader(valDatasetR, batch_size=testBatchSize, **kwargs)\n",
        "\n",
        "train_loaderAll = torch.utils.data.DataLoader(trainDatasetAll, batch_size=batchSize, **kwargs)\n",
        "test_loaderAll = torch.utils.data.DataLoader(valDatasetAll, batch_size=testBatchSize, **kwargs)\n",
        "\n",
        "\n",
        "modelC = ModelClass().to(device)\n",
        "opt = torch.optim.Adam(modelC.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  trainClass(modelC,device,train_loaderC,opt,epoch,200)\n",
        "  testClass(modelC,device,test_loaderC,epoch,200)\n",
        "\n",
        "''' # REGRESSION MODEL\n",
        "modelR = ModelReg().to(device)\n",
        "opt = torch.optim.Adam(modelR.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  trainReg(modelR,device,train_loaderR,opt,epoch,200)\n",
        "  testReg(modelR,device,test_loaderR,epoch,200)\n",
        "'''\n",
        "\n",
        "''' # FINAL, COMBINED MODEL\n",
        "modelA = ModelCombined().to(device)\n",
        "opt = torch.optim.Adam(modelA.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(100):\n",
        "  train(modelA,device,train_loaderAll,opt,epoch,200)\n",
        "  test(modelA,device,test_loaderAll,epoch,200)\n",
        "'''"
      ],
      "metadata": {
        "id": "Noooak7Y18Bx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6f7faf5-8536-4510-be32-20f096807490"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0, [0/8000 images (0%)] \t Partial Loss: 5.197415\n",
            "Test loss: 0.004879971966147423, For classification: 16 / 2000\n",
            "Train Epoch: 1, [0/8000 images (0%)] \t Partial Loss: 5.133063\n",
            "Test loss: 0.0048189968802034855, For classification: 21 / 2000\n",
            "Train Epoch: 2, [0/8000 images (0%)] \t Partial Loss: 4.897349\n",
            "Test loss: 0.0047516366466879845, For classification: 15 / 2000\n",
            "Train Epoch: 3, [0/8000 images (0%)] \t Partial Loss: 4.926280\n",
            "Test loss: 0.004722355864942074, For classification: 27 / 2000\n",
            "Train Epoch: 4, [0/8000 images (0%)] \t Partial Loss: 4.779860\n",
            "Test loss: 0.004663263447582722, For classification: 32 / 2000\n",
            "Train Epoch: 5, [0/8000 images (0%)] \t Partial Loss: 4.739948\n",
            "Test loss: 0.004730710759758949, For classification: 14 / 2000\n",
            "Train Epoch: 6, [0/8000 images (0%)] \t Partial Loss: 4.660669\n",
            "Test loss: 0.004360463470220566, For classification: 37 / 2000\n",
            "Train Epoch: 7, [0/8000 images (0%)] \t Partial Loss: 4.442240\n",
            "Test loss: 0.004128515254706144, For classification: 45 / 2000\n",
            "Train Epoch: 8, [0/8000 images (0%)] \t Partial Loss: 4.324409\n",
            "Test loss: 0.003992581740021706, For classification: 47 / 2000\n",
            "Train Epoch: 9, [0/8000 images (0%)] \t Partial Loss: 4.069738\n",
            "Test loss: 0.003906183410435915, For classification: 65 / 2000\n",
            "Train Epoch: 10, [0/8000 images (0%)] \t Partial Loss: 3.978966\n",
            "Test loss: 0.0038595194928348064, For classification: 87 / 2000\n",
            "Train Epoch: 11, [0/8000 images (0%)] \t Partial Loss: 4.083090\n",
            "Test loss: 0.003694737097248435, For classification: 98 / 2000\n",
            "Train Epoch: 12, [0/8000 images (0%)] \t Partial Loss: 3.934416\n",
            "Test loss: 0.0035223881714046, For classification: 144 / 2000\n",
            "Train Epoch: 13, [0/8000 images (0%)] \t Partial Loss: 3.825251\n",
            "Test loss: 0.003275653813034296, For classification: 215 / 2000\n",
            "Train Epoch: 14, [0/8000 images (0%)] \t Partial Loss: 3.620655\n",
            "Test loss: 0.0031046911608427763, For classification: 263 / 2000\n",
            "Train Epoch: 15, [0/8000 images (0%)] \t Partial Loss: 3.418788\n",
            "Test loss: 0.0030266318935900927, For classification: 246 / 2000\n",
            "Train Epoch: 16, [0/8000 images (0%)] \t Partial Loss: 3.538514\n",
            "Test loss: 0.0030775293707847595, For classification: 229 / 2000\n",
            "Train Epoch: 17, [0/8000 images (0%)] \t Partial Loss: 3.567374\n",
            "Test loss: 0.0030003541614860296, For classification: 253 / 2000\n",
            "Train Epoch: 18, [0/8000 images (0%)] \t Partial Loss: 3.502011\n",
            "Test loss: 0.002824559574946761, For classification: 306 / 2000\n",
            "Train Epoch: 19, [0/8000 images (0%)] \t Partial Loss: 3.296517\n",
            "Test loss: 0.0027361824177205563, For classification: 361 / 2000\n",
            "Train Epoch: 20, [0/8000 images (0%)] \t Partial Loss: 3.122685\n",
            "Test loss: 0.0027079773135483265, For classification: 344 / 2000\n",
            "Train Epoch: 21, [0/8000 images (0%)] \t Partial Loss: 3.241896\n",
            "Test loss: 0.0026329613756388426, For classification: 410 / 2000\n",
            "Train Epoch: 22, [0/8000 images (0%)] \t Partial Loss: 3.208192\n",
            "Test loss: 0.0031904494389891624, For classification: 195 / 2000\n",
            "Train Epoch: 23, [0/8000 images (0%)] \t Partial Loss: 3.302716\n",
            "Test loss: 0.002539257751777768, For classification: 442 / 2000\n",
            "Train Epoch: 24, [0/8000 images (0%)] \t Partial Loss: 3.048769\n",
            "Test loss: 0.0025278967805206776, For classification: 398 / 2000\n",
            "Train Epoch: 25, [0/8000 images (0%)] \t Partial Loss: 3.070887\n",
            "Test loss: 0.0022560530342161655, For classification: 557 / 2000\n",
            "Train Epoch: 26, [0/8000 images (0%)] \t Partial Loss: 2.843762\n",
            "Test loss: 0.0021536480635404587, For classification: 610 / 2000\n",
            "Train Epoch: 27, [0/8000 images (0%)] \t Partial Loss: 2.858348\n",
            "Test loss: 0.0020655139815062284, For classification: 594 / 2000\n",
            "Train Epoch: 28, [0/8000 images (0%)] \t Partial Loss: 2.709600\n",
            "Test loss: 0.0020585309248417616, For classification: 576 / 2000\n",
            "Train Epoch: 29, [0/8000 images (0%)] \t Partial Loss: 2.691959\n",
            "Test loss: 0.0020641719456762075, For classification: 564 / 2000\n",
            "Train Epoch: 30, [0/8000 images (0%)] \t Partial Loss: 2.811356\n",
            "Test loss: 0.0018937526037916541, For classification: 665 / 2000\n",
            "Train Epoch: 31, [0/8000 images (0%)] \t Partial Loss: 2.583222\n",
            "Test loss: 0.0019421058241277933, For classification: 609 / 2000\n",
            "Train Epoch: 32, [0/8000 images (0%)] \t Partial Loss: 2.395171\n",
            "Test loss: 0.0017797315958887339, For classification: 712 / 2000\n",
            "Train Epoch: 33, [0/8000 images (0%)] \t Partial Loss: 2.625168\n",
            "Test loss: 0.0018082159804180264, For classification: 677 / 2000\n",
            "Train Epoch: 34, [0/8000 images (0%)] \t Partial Loss: 2.442840\n",
            "Test loss: 0.001839906326495111, For classification: 632 / 2000\n",
            "Train Epoch: 35, [0/8000 images (0%)] \t Partial Loss: 2.373024\n",
            "Test loss: 0.0016699166735634208, For classification: 769 / 2000\n",
            "Train Epoch: 36, [0/8000 images (0%)] \t Partial Loss: 2.322237\n",
            "Test loss: 0.0016990955919027328, For classification: 719 / 2000\n",
            "Train Epoch: 37, [0/8000 images (0%)] \t Partial Loss: 2.274662\n",
            "Test loss: 0.0016794675029814243, For classification: 699 / 2000\n",
            "Train Epoch: 38, [0/8000 images (0%)] \t Partial Loss: 2.297525\n",
            "Test loss: 0.0017768118996173143, For classification: 647 / 2000\n",
            "Train Epoch: 39, [0/8000 images (0%)] \t Partial Loss: 2.238479\n",
            "Test loss: 0.0016169636510312557, For classification: 770 / 2000\n",
            "Train Epoch: 40, [0/8000 images (0%)] \t Partial Loss: 2.481362\n",
            "Test loss: 0.0016200030222535133, For classification: 776 / 2000\n",
            "Train Epoch: 41, [0/8000 images (0%)] \t Partial Loss: 2.088140\n",
            "Test loss: 0.0015764067647978663, For classification: 804 / 2000\n",
            "Train Epoch: 42, [0/8000 images (0%)] \t Partial Loss: 2.160836\n",
            "Test loss: 0.001612378633581102, For classification: 740 / 2000\n",
            "Train Epoch: 43, [0/8000 images (0%)] \t Partial Loss: 2.056292\n",
            "Test loss: 0.001526815933175385, For classification: 802 / 2000\n",
            "Train Epoch: 44, [0/8000 images (0%)] \t Partial Loss: 2.223589\n",
            "Test loss: 0.0016141290543600917, For classification: 748 / 2000\n",
            "Train Epoch: 45, [0/8000 images (0%)] \t Partial Loss: 2.033359\n",
            "Test loss: 0.001490113907493651, For classification: 798 / 2000\n",
            "Train Epoch: 46, [0/8000 images (0%)] \t Partial Loss: 2.044419\n",
            "Test loss: 0.0015873004449531436, For classification: 727 / 2000\n",
            "Train Epoch: 47, [0/8000 images (0%)] \t Partial Loss: 2.160415\n",
            "Test loss: 0.0016159182414412498, For classification: 730 / 2000\n",
            "Train Epoch: 48, [0/8000 images (0%)] \t Partial Loss: 2.096333\n",
            "Test loss: 0.0014926391886547208, For classification: 807 / 2000\n",
            "Train Epoch: 49, [0/8000 images (0%)] \t Partial Loss: 1.947821\n",
            "Test loss: 0.0015981706092134118, For classification: 713 / 2000\n",
            "Train Epoch: 50, [0/8000 images (0%)] \t Partial Loss: 2.229752\n",
            "Test loss: 0.0014641955494880676, For classification: 837 / 2000\n",
            "Train Epoch: 51, [0/8000 images (0%)] \t Partial Loss: 2.053201\n",
            "Test loss: 0.0015255025355145335, For classification: 794 / 2000\n",
            "Train Epoch: 52, [0/8000 images (0%)] \t Partial Loss: 2.011469\n",
            "Test loss: 0.0015327960718423128, For classification: 793 / 2000\n",
            "Train Epoch: 53, [0/8000 images (0%)] \t Partial Loss: 2.023813\n",
            "Test loss: 0.001513141905888915, For classification: 805 / 2000\n",
            "Train Epoch: 54, [0/8000 images (0%)] \t Partial Loss: 2.124337\n",
            "Test loss: 0.0015001472784206271, For classification: 795 / 2000\n",
            "Train Epoch: 55, [0/8000 images (0%)] \t Partial Loss: 2.051563\n",
            "Test loss: 0.0014991182833909988, For classification: 790 / 2000\n",
            "Train Epoch: 56, [0/8000 images (0%)] \t Partial Loss: 1.923237\n",
            "Test loss: 0.0015340214595198631, For classification: 781 / 2000\n",
            "Train Epoch: 57, [0/8000 images (0%)] \t Partial Loss: 1.801232\n",
            "Test loss: 0.0015293655451387167, For classification: 765 / 2000\n",
            "Train Epoch: 58, [0/8000 images (0%)] \t Partial Loss: 1.911955\n",
            "Test loss: 0.0016003946075215936, For classification: 748 / 2000\n",
            "Train Epoch: 59, [0/8000 images (0%)] \t Partial Loss: 1.789923\n",
            "Test loss: 0.0014759311452507973, For classification: 810 / 2000\n",
            "Train Epoch: 60, [0/8000 images (0%)] \t Partial Loss: 1.944515\n",
            "Test loss: 0.0015489080687984824, For classification: 766 / 2000\n",
            "Train Epoch: 61, [0/8000 images (0%)] \t Partial Loss: 1.758804\n",
            "Test loss: 0.0014732596464455128, For classification: 822 / 2000\n",
            "Train Epoch: 62, [0/8000 images (0%)] \t Partial Loss: 1.977750\n",
            "Test loss: 0.0015253945020958781, For classification: 776 / 2000\n",
            "Train Epoch: 63, [0/8000 images (0%)] \t Partial Loss: 2.037049\n",
            "Test loss: 0.0015722083626314998, For classification: 776 / 2000\n",
            "Train Epoch: 64, [0/8000 images (0%)] \t Partial Loss: 1.806046\n",
            "Test loss: 0.001495524076744914, For classification: 818 / 2000\n",
            "Train Epoch: 65, [0/8000 images (0%)] \t Partial Loss: 1.600011\n",
            "Test loss: 0.0015752615872770548, For classification: 785 / 2000\n",
            "Train Epoch: 66, [0/8000 images (0%)] \t Partial Loss: 1.826834\n",
            "Test loss: 0.0014806156978011131, For classification: 854 / 2000\n",
            "Train Epoch: 67, [0/8000 images (0%)] \t Partial Loss: 1.803241\n",
            "Test loss: 0.001496321288868785, For classification: 836 / 2000\n",
            "Train Epoch: 68, [0/8000 images (0%)] \t Partial Loss: 1.881097\n",
            "Test loss: 0.0014752564020454884, For classification: 800 / 2000\n",
            "Train Epoch: 69, [0/8000 images (0%)] \t Partial Loss: 1.647740\n",
            "Test loss: 0.0014899063389748335, For classification: 775 / 2000\n",
            "Train Epoch: 70, [0/8000 images (0%)] \t Partial Loss: 1.904566\n",
            "Test loss: 0.001465031411498785, For classification: 842 / 2000\n",
            "Train Epoch: 71, [0/8000 images (0%)] \t Partial Loss: 1.885908\n",
            "Test loss: 0.0014426601119339466, For classification: 835 / 2000\n",
            "Train Epoch: 72, [0/8000 images (0%)] \t Partial Loss: 1.637303\n",
            "Test loss: 0.0015245360555127263, For classification: 792 / 2000\n",
            "Train Epoch: 73, [0/8000 images (0%)] \t Partial Loss: 1.780111\n",
            "Test loss: 0.0014826715923845768, For classification: 807 / 2000\n",
            "Train Epoch: 74, [0/8000 images (0%)] \t Partial Loss: 1.736712\n",
            "Test loss: 0.0014775394229218364, For classification: 849 / 2000\n",
            "Train Epoch: 75, [0/8000 images (0%)] \t Partial Loss: 1.566476\n",
            "Test loss: 0.001461526146158576, For classification: 827 / 2000\n",
            "Train Epoch: 76, [0/8000 images (0%)] \t Partial Loss: 1.706669\n",
            "Test loss: 0.0014493624912574887, For classification: 856 / 2000\n",
            "Train Epoch: 77, [0/8000 images (0%)] \t Partial Loss: 1.749325\n",
            "Test loss: 0.001487301429733634, For classification: 854 / 2000\n",
            "Train Epoch: 78, [0/8000 images (0%)] \t Partial Loss: 1.737087\n",
            "Test loss: 0.001472840434871614, For classification: 832 / 2000\n",
            "Train Epoch: 79, [0/8000 images (0%)] \t Partial Loss: 1.884703\n",
            "Test loss: 0.0014664297923445702, For classification: 855 / 2000\n",
            "Train Epoch: 80, [0/8000 images (0%)] \t Partial Loss: 1.738674\n",
            "Test loss: 0.0015708826249465346, For classification: 792 / 2000\n",
            "Train Epoch: 81, [0/8000 images (0%)] \t Partial Loss: 1.824593\n",
            "Test loss: 0.0015389092732220888, For classification: 774 / 2000\n",
            "Train Epoch: 82, [0/8000 images (0%)] \t Partial Loss: 1.634410\n",
            "Test loss: 0.0015486779157072306, For classification: 820 / 2000\n",
            "Train Epoch: 83, [0/8000 images (0%)] \t Partial Loss: 1.450874\n",
            "Test loss: 0.0014909006422385573, For classification: 878 / 2000\n",
            "Train Epoch: 84, [0/8000 images (0%)] \t Partial Loss: 1.803757\n",
            "Test loss: 0.0014809005660936236, For classification: 830 / 2000\n",
            "Train Epoch: 85, [0/8000 images (0%)] \t Partial Loss: 1.435056\n",
            "Test loss: 0.0017217651475220919, For classification: 788 / 2000\n",
            "Train Epoch: 86, [0/8000 images (0%)] \t Partial Loss: 1.867382\n",
            "Test loss: 0.0017115863738581538, For classification: 755 / 2000\n",
            "Train Epoch: 87, [0/8000 images (0%)] \t Partial Loss: 1.690739\n",
            "Test loss: 0.0015436792746186256, For classification: 806 / 2000\n",
            "Train Epoch: 88, [0/8000 images (0%)] \t Partial Loss: 1.446456\n",
            "Test loss: 0.001499017933383584, For classification: 837 / 2000\n",
            "Train Epoch: 89, [0/8000 images (0%)] \t Partial Loss: 1.840407\n",
            "Test loss: 0.0015770954778417945, For classification: 787 / 2000\n",
            "Train Epoch: 90, [0/8000 images (0%)] \t Partial Loss: 1.384897\n",
            "Test loss: 0.001555963302962482, For classification: 861 / 2000\n",
            "Train Epoch: 91, [0/8000 images (0%)] \t Partial Loss: 1.521609\n",
            "Test loss: 0.0015145750949159265, For classification: 858 / 2000\n",
            "Train Epoch: 92, [0/8000 images (0%)] \t Partial Loss: 1.647283\n",
            "Test loss: 0.001541675184853375, For classification: 800 / 2000\n",
            "Train Epoch: 93, [0/8000 images (0%)] \t Partial Loss: 1.805470\n",
            "Test loss: 0.0015279194340109825, For classification: 863 / 2000\n",
            "Train Epoch: 94, [0/8000 images (0%)] \t Partial Loss: 1.827930\n",
            "Test loss: 0.0015426540048792958, For classification: 780 / 2000\n",
            "Train Epoch: 95, [0/8000 images (0%)] \t Partial Loss: 1.584768\n",
            "Test loss: 0.001554063055664301, For classification: 850 / 2000\n",
            "Train Epoch: 96, [0/8000 images (0%)] \t Partial Loss: 1.485337\n",
            "Test loss: 0.0015175675507634878, For classification: 829 / 2000\n",
            "Train Epoch: 97, [0/8000 images (0%)] \t Partial Loss: 1.464105\n",
            "Test loss: 0.0015940772136673331, For classification: 820 / 2000\n",
            "Train Epoch: 98, [0/8000 images (0%)] \t Partial Loss: 1.548606\n",
            "Test loss: 0.0015117962611839175, For classification: 838 / 2000\n",
            "Train Epoch: 99, [0/8000 images (0%)] \t Partial Loss: 1.685813\n",
            "Test loss: 0.0015503247268497944, For classification: 824 / 2000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' # FINAL, COMBINED MODEL\\nmodelA = ModelCombined().to(device)\\nopt = torch.optim.Adam(modelA.parameters(), lr=0.001)\\n\\nfor epoch in range(100):\\n  train(modelA,device,train_loaderAll,opt,epoch,200)\\n  test(modelA,device,test_loaderAll,epoch,200)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}