{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration strategies for DQN algorithm\n",
        "\n",
        "In this assignment we are interested in exploration strategies that can be combined with Q-learning.\n",
        "Q-learning is an off-policy algorithm, which means that the data for the algorithm can be collected by a different policy (called behavioural policy) that the one the algorithm learns.\n",
        "\n",
        "Here we come across a classical trade-off in reinforcement learning, called exploration-exploitation trade-off. On the one hand, our behavioural policy should try out new state-action pairs to gain knowledge about their returns. On the other hand, when our estimate of returns is good enough, we would like to follow the state-action pairs with the highest estimated returns.\n",
        "\n",
        "We will be operating on DQN [(Mnih 2014)](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) algorithm and analyzing epsilon-greedy strategy, boltzmann and max-boltzmann strategy and combination of epsilon-greedy and boltzmann.\n",
        "We evaluate performance of DQN variants on the Lunar Lander environment.\n",
        "\n",
        "We provide an implementation of the DQN algorithm with random exploration strategy.\n",
        "Your goal is to implement the exploration variants by overriding appropriate methods of the provided class.\n"
      ],
      "metadata": {
        "id": "OCcuj7kCPowC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading\n",
        "\n",
        "To obtain the points for the assignment You need to provide the implementation of exploration techniques AND report with plots and conclusions.\n",
        "Measuring sensitivity means that You should at least examine one reasonably lower and one reasonably greater value of the considered hyperparameter (or the pair of hyperparameters).\n",
        "\n",
        "\n",
        "1. Implement epsilon-greedy strategy and investigate hyperparameter sensitivity (1 point).\n",
        "2. Implement epsilon-greedy strategy with epsilon annealing and investigate hyperparameter sensitivity (1 point).\n",
        "3. Implement boltzmann strategy and investigate hyperparameter sensitivity (1 point).\n",
        "4. Implement boltzmann strategy with temperature annealing and investigate hyperparameter sensitivity (1 point).\n",
        "5. Implement max-boltzmann strategy and investigate hyperparameter sensitivity (1 point).\n",
        "6. Implement max-boltzmann strategy with temperature annealing and investigate hyperparameter sensitivity (1 point).\n",
        "7. Implement combination of epsilon-greedy with epsilon annealing and boltzmann strategy and investigate hyperparameter sensitivity (1 point)\n",
        "8. (*) Bonus: propose another reasonable approach to combine epsilon-greedy with epsilon annealing strategy and boltzmann strategy and/or another reasonable strategy of temperature annealing for the boltzmann strategy (2 points).\n",
        "9. Compare methods, present plots and conclusions in a clear manner (3 points).\n",
        "\n",
        "You can obtain max 10 points, bonus points increase Your score, if You lose points in some other tasks."
      ],
      "metadata": {
        "id": "MdeRz_jnUBmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import necessary libraries."
      ],
      "metadata": {
        "id": "WugRYf0FYsxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "P3Ynynd4GGKO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f0e418e-6ea1-4eef-8c88-4ebd822a20f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d==2.3.10 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.3.10)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KKd6_LiXdh3D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we set hyperparameters of the training, set seeds for reproducibility and set weights initialization.\n",
        "Although for debugging it might be useful to operate on a smaller number of training_steps, seeds etc., in the final evaluation DO NOT CHANGE these parameters."
      ],
      "metadata": {
        "id": "kgtGd69HY7PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class parse_args:\n",
        "  def __init__(self):\n",
        "    self.n_seeds = 6\n",
        "    self.n_evaluate_episodes = 5\n",
        "    self.n_training_steps = 100000 # orignially 100000\n",
        "    self.buffer_size = 10000\n",
        "    self.init_steps = 10000\n",
        "    self.target_update_freq = 50\n",
        "    self.eval_freq = 1000\n",
        "    self.gym_id = \"LunarLander-v3\"\n",
        "    env = gym.make(self.gym_id)\n",
        "    self.state_dim = env.observation_space.shape[0]\n",
        "    self.batch_size = 128\n",
        "    self.hidden_dim = 128\n",
        "    self.action_dim = env.action_space.n\n",
        "    self.discount = 0.99\n",
        "    self.lr = 7e-4\n",
        "    self.cuda = True\n",
        "    self.device = torch.device(\"cuda\" if torch.cuda.is_available() and self.cuda else \"cpu\")\n",
        "\n",
        "args = parse_args()\n",
        "first_half_training_args = parse_args()\n",
        "first_half_training_args.n_training_steps = first_half_training_args.n_training_steps // 2\n",
        "second_half_training_args = parse_args()\n",
        "second_half_training_args.n_training_steps = second_half_training_args.n_training_steps // 2\n",
        "second_half_training_args.init_steps = 1"
      ],
      "metadata": {
        "id": "AbD6x1GJFgqr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "def weight_init(model):\n",
        "  torch.nn.init.orthogonal_(model.weight.data)\n",
        "  model.bias.data.fill_(0.0)"
      ],
      "metadata": {
        "id": "wJ4x_6u9fQXA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement the replay buffer.\n",
        "It has two methods: add one transition to the buffer and sample batch of transitions from the buffer."
      ],
      "metadata": {
        "id": "4nk9OsHCaPZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  def __init__(self, args):\n",
        "    self.states = np.zeros((args.buffer_size, args.n_seeds, args.state_dim), dtype = np.float32)\n",
        "    self.actions = np.zeros((args.buffer_size, args.n_seeds), dtype = np.int64)\n",
        "    self.rewards = np.zeros((args.buffer_size, args.n_seeds), dtype = np.float32)\n",
        "    self.next_states = np.zeros((args.buffer_size, args.n_seeds, args.state_dim), dtype = np.float32)\n",
        "    self.terminals = np.zeros((args.buffer_size, args.n_seeds), dtype = np.int64)\n",
        "    self.idx = 0\n",
        "    self.current_size = 0\n",
        "    self.args = args\n",
        "\n",
        "  def add(self, state, action, reward, next_state, terminal):\n",
        "    if self.current_size < self.args.buffer_size:\n",
        "      self.current_size += 1\n",
        "    self.states[self.idx, :, :] = state\n",
        "    self.actions[self.idx, :] = action\n",
        "    self.rewards[self.idx, :] = reward\n",
        "    self.next_states[self.idx, :, :] = next_state\n",
        "    self.terminals[self.idx, :] = terminal\n",
        "    self.idx = (self.idx + 1) % self.args.buffer_size\n",
        "\n",
        "  def sample(self):\n",
        "    sample_idxs = np.random.permutation(self.current_size)[:self.args.batch_size]\n",
        "    states = torch.from_numpy(self.states[sample_idxs]).to(self.args.device)\n",
        "    actions = torch.from_numpy(self.actions[sample_idxs]).to(self.args.device)\n",
        "    rewards = torch.from_numpy(self.rewards[sample_idxs]).to(self.args.device)\n",
        "    next_states = torch.from_numpy(self.next_states[sample_idxs]).to(self.args.device)\n",
        "    terminals = torch.from_numpy(self.terminals[sample_idxs]).to(self.args.device)\n",
        "\n",
        "    return states, actions, rewards, next_states, terminals\n"
      ],
      "metadata": {
        "id": "8kswZi26I9_t"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement a simple Q network architecture with three layers and ReLU activations."
      ],
      "metadata": {
        "id": "WI1MmteualVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(torch.nn.Module):\n",
        "  def __init__(self, args):\n",
        "    super(QNetwork, self).__init__()\n",
        "    self.layer_1 = torch.nn.Linear(args.state_dim, args.hidden_dim)\n",
        "    self.layer_2 = torch.nn.Linear(args.hidden_dim, args.hidden_dim)\n",
        "    self.layer_3 = torch.nn.Linear(args.hidden_dim, args.action_dim)\n",
        "    self.relu = torch.nn.ReLU()\n",
        "\n",
        "    self.layer_1.apply(weight_init)\n",
        "    self.layer_2.apply(weight_init)\n",
        "    self.layer_3.apply(weight_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.layer_1(x))\n",
        "    x = self.relu(self.layer_2(x))\n",
        "    x = self.layer_3(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "jXq66twrc2Mh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we provide code for DQN with random exploration."
      ],
      "metadata": {
        "id": "NB2SjjLldglT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "myEn1aKdaURQ"
      },
      "outputs": [],
      "source": [
        "TRAIN_SEED = 0\n",
        "EVAL_SEED = 1\n",
        "\n",
        "class DQN:\n",
        "  def __init__(self, args):\n",
        "    self.args = args\n",
        "    self.discount = self.args.discount\n",
        "    self.reset()\n",
        "    self.annealing = False\n",
        "\n",
        "  # Copying parameters of other DQN class by reference (for half epsion-greedy, half boltzmann task)\n",
        "  def copy_reference(self, other):\n",
        "    self.buffer = other.buffer\n",
        "    self.q_net = other.q_net\n",
        "    self.q_target = other.q_target\n",
        "    self.optimizer = other.optimizer\n",
        "\n",
        "  # Annealing of epsilon and/or temperature\n",
        "  def anneal(self, step):\n",
        "    pass\n",
        "\n",
        "  # Greedy action\n",
        "  def get_greedy_action(self, states):\n",
        "    with torch.no_grad():\n",
        "      action = torch.argmax(self.q_net(states), dim = -1).detach().cpu().numpy()\n",
        "      return action\n",
        "\n",
        "  # Exploration action choice\n",
        "  def explore(self, states):\n",
        "    # Random action choice\n",
        "    action = np.random.randint(self.args.action_dim, size = self.args.n_seeds)\n",
        "    return action\n",
        "\n",
        "  # Update of the main critic\n",
        "  def update(self):\n",
        "    states, actions, rewards, next_states, terminals = self.buffer.sample()\n",
        "    with torch.no_grad():\n",
        "      q_next_states = torch.max(self.q_target(next_states), dim = -1)[0]\n",
        "    ones_tensor = torch.ones_like(terminals).to(self.args.device)\n",
        "    targets = rewards + (ones_tensor - terminals) * self.discount * q_next_states\n",
        "\n",
        "    self.optimizer.zero_grad()\n",
        "    q_values = self.q_net(states).gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "    loss = torch.mean((q_values - targets) ** 2)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  # Update of the targer critic\n",
        "  def update_target(self):\n",
        "    self.q_target.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "  # Evaluation of the performance on test environments.\n",
        "  def evaluate(self):\n",
        "    eval_results = np.zeros(self.args.n_seeds)\n",
        "    with torch.no_grad():\n",
        "      eval_env = gym.make_vec(self.args.gym_id, num_envs = self.args.n_seeds, vectorization_mode=\"sync\")\n",
        "      eval_env.reset(seed = EVAL_SEED)\n",
        "      for _ in range(self.args.n_evaluate_episodes):\n",
        "        state, info = eval_env.reset()\n",
        "        episode_reward = np.zeros(self.args.n_seeds)\n",
        "        mask = np.ones(self.args.n_seeds)\n",
        "        while np.sum(mask) > 0:\n",
        "          action = self.get_greedy_action(torch.tensor(state).to(self.args.device))\n",
        "          next_state, reward, terminal, truncated, _ = eval_env.step(action)\n",
        "          episode_reward += mask * reward\n",
        "          state = next_state\n",
        "          mask *= (np.ones(self.args.n_seeds) - terminal) * (np.ones(self.args.n_seeds) - truncated)\n",
        "        eval_results += episode_reward / self.args.n_evaluate_episodes\n",
        "    return np.mean(eval_results), np.std(eval_results)\n",
        "\n",
        "\n",
        "  # Resetting the algorithm\n",
        "  def reset(self):\n",
        "    self.buffer = ReplayBuffer(self.args)\n",
        "    self.q_net = QNetwork(self.args).to(self.args.device) # main critic\n",
        "    self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr = self.args.lr, eps = 1e-5)\n",
        "    self.q_target = QNetwork(self.args).to(self.args.device) # target critic\n",
        "    self.update_target()\n",
        "\n",
        "  # Training loop\n",
        "  def train(self):\n",
        "    eval_results_means = np.array([])\n",
        "    eval_results_stds = np.array([])\n",
        "    train_env = gym.make_vec(self.args.gym_id, num_envs = self.args.n_seeds, vectorization_mode=\"sync\")\n",
        "    state, info = train_env.reset(seed = TRAIN_SEED)\n",
        "    mask = np.ones(self.args.n_seeds)\n",
        "    for step in range(self.args.n_training_steps):\n",
        "      action = self.explore(torch.tensor(state).unsqueeze(0).to(self.args.device))\n",
        "      if self.annealing:\n",
        "        self.anneal(step)\n",
        "      next_state, reward, terminal, truncated, _ = train_env.step(action)\n",
        "      self.buffer.add(state, action, reward, next_state, terminal)\n",
        "      state = next_state\n",
        "      if step % self.args.eval_freq == 0:\n",
        "          print(f\"Training step: {step}\")\n",
        "          eval_mean, eval_std = self.evaluate()\n",
        "          print(f\"Eval mean: {eval_mean}; eval_std: {eval_std}\")\n",
        "          eval_results_means = np.append(eval_results_means, eval_mean)\n",
        "          eval_results_stds = np.append(eval_results_stds, eval_std)\n",
        "      if step >= self.args.init_steps:\n",
        "        self.update()\n",
        "        if step % self.args.target_update_freq == 0:\n",
        "          self.update_target()\n",
        "      mask *= (np.ones(self.args.n_seeds) - terminal) * (np.ones(self.args.n_seeds) - truncated)\n",
        "      if np.sum(mask) == 0:\n",
        "        state, info = train_env.reset()\n",
        "        mask = np.ones(self.args.n_seeds)\n",
        "\n",
        "    return eval_results_means, eval_results_stds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we implement functions for plotting."
      ],
      "metadata": {
        "id": "2JDGKsz8chmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth(data, weigth = 0.9):\n",
        "  smooth_data = np.copy(data)\n",
        "  for index in range(1, len(data)):\n",
        "    smooth_data[index] = smooth_data[index - 1] * weigth + data[index] * (1.0 - weigth)\n",
        "\n",
        "  return smooth_data\n",
        "\n",
        "def plot_smooth(args, result_means, result_stds):\n",
        "  smooth_result_means = smooth(result_means)\n",
        "  smooth_result_stds = smooth(result_stds)\n",
        "  print(smooth_result_means)\n",
        "  print(smooth_result_stds)\n",
        "  xs = np.arange(len(result_means)) * args.eval_freq\n",
        "  print(xs)\n",
        "  plt.plot(xs, smooth_result_means, color = \"blue\")\n",
        "  plt.fill_between(xs, smooth_result_means - smooth_result_stds, smooth_result_means + smooth_result_stds, alpha = 0.2, label = \"smoothed_rewards\")\n",
        "  plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "def plot_smooth_many(args, result_means_list, result_stds_list, names_list, colours_list):\n",
        "  plt.figure(figsize=(12.8, 9.6))\n",
        "  for result_means, result_stds, name, colour in zip(result_means_list, result_stds_list, names_list, colours_list):\n",
        "    smooth_result_means = smooth(result_means)\n",
        "    smooth_result_stds = smooth(result_stds)\n",
        "    print(smooth_result_means)\n",
        "    print(smooth_result_stds)\n",
        "    xs = np.arange(len(result_means)) * args.eval_freq\n",
        "    print(xs)\n",
        "    plt.plot(xs, smooth_result_means, color = colour)\n",
        "    plt.fill_between(xs, smooth_result_means - smooth_result_stds, smooth_result_means + smooth_result_stds, alpha = 0.2, color = colour, label = f\"smoothed_rewards_{name}\")\n",
        "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "  plt.show()\n",
        "  plt.clf()\n",
        "\n",
        "def plot_results(result_mean, result_std):\n",
        "  plot_smooth(args, result_mean, result_std)\n",
        "\n",
        "def plot_results_many(result_means_list, result_stds_list, name_list, colours_list):\n",
        "  plot_smooth_many(args, result_means_list, result_stds_list, name_list, colours_list)"
      ],
      "metadata": {
        "id": "P_zLGVqHpvQz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we provide code for training across different random seeds."
      ],
      "metadata": {
        "id": "hoiLn1y1cshT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dqn(dqn):\n",
        "  set_seed(TRAIN_SEED)\n",
        "  dqn.reset()\n",
        "  result_mean, result_std = dqn.train()\n",
        "  print(result_mean)\n",
        "  return result_mean, result_std\n"
      ],
      "metadata": {
        "id": "XLG6xzJ-o7oN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN(args)\n",
        "result_means_dqn, result_stds_dqn = train_dqn(dqn)\n",
        "plot_results(result_means_dqn, result_stds_dqn)"
      ],
      "metadata": {
        "id": "R2PyyNFdq50-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "322b7f37-ad7a-487e-8e80-15a804ab201d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step: 0\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 1000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 2000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 3000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 4000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 5000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 6000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 7000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 8000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 9000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 10000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 11000\n",
            "Eval mean: -92.09635353225616; eval_std: 7.427149523591686\n",
            "Training step: 12000\n",
            "Eval mean: -139.72535441078307; eval_std: 13.111407545817277\n",
            "Training step: 13000\n",
            "Eval mean: -156.57203515448404; eval_std: 12.28387573614405\n",
            "Training step: 14000\n",
            "Eval mean: -175.94877050760792; eval_std: 16.398050405635022\n",
            "Training step: 15000\n",
            "Eval mean: -205.06963345201322; eval_std: 6.871300559291905\n",
            "Training step: 16000\n",
            "Eval mean: -208.95670551463294; eval_std: 31.192847539442678\n",
            "Training step: 17000\n",
            "Eval mean: -183.3771004045324; eval_std: 18.889041708542244\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2355416663.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_means_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_means_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-503547981.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(dqn)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186968863.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m           \u001b[0meval_results_stds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results_stds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186968863.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;31m# Update of the targer critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Optimizer.step#{self.__class__.__name__}.step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m                 \u001b[0;31m# call optimizer step pre hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 for pre_hook in chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/profiler.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunctionSubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RecordFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_function_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m                 )\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     def _dispatch_in_python(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the goal is to implement the epsilon-gredy strategy. With probability epsilon we choose uniformly a random action and with probability 1-epsilon we take the action with the highest Q-value according to the main critic."
      ],
      "metadata": {
        "id": "8BHzDjEUfYn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class EpsilonGreedyDQN(DQN):\n",
        "  def __init__(self, args):\n",
        "    super(EpsilonGreedyDQN, self).__init__(args)\n",
        "    self.epsilon = 0.1 # investigate sensitivity\n",
        "\n",
        "  def explore(self, states):\n",
        "    action = None\n",
        "    # TODO\n",
        "    ran = random.random()\n",
        "    #print(\"random: \", ran)\n",
        "    if ran < self.epsilon:\n",
        "    # probably do the first step as epsilon chance\n",
        "      action = np.random.randint(self.args.action_dim, size = self.args.n_seeds)\n",
        "      #print(action)\n",
        "      #(\"Random! \")\n",
        "    else:\n",
        "      #(\"Random bez epsilon\")\n",
        "      # do a argmax over qvalues\n",
        "\n",
        "      action = self.get_greedy_action(states).squeeze()\n",
        "\n",
        "\n",
        "      #print(\"Actions: \", action)\n",
        "    ####################################\n",
        "    ####################################\n",
        "    return action"
      ],
      "metadata": {
        "id": "v6lj71xe7c77"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_greedy_dqn = EpsilonGreedyDQN(args)\n",
        "result_means_epsilon_greedy_dqn, result_stds_epsilon_greedy_dqn = train_dqn(epsilon_greedy_dqn)\n",
        "plot_results(result_means_epsilon_greedy_dqn, result_stds_epsilon_greedy_dqn)"
      ],
      "metadata": {
        "id": "JefbwjjR9AU-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "f1c621e9-9367-49bb-fd6d-5c682d5c1d61"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "tensor(1) (<class 'torch.Tensor'>) invalid ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1818516985.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepsilon_greedy_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedyDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_means_epsilon_greedy_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_epsilon_greedy_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_greedy_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_means_epsilon_greedy_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_epsilon_greedy_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-503547981.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(dqn)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186968863.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannealing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manneal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m       \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/vector/sync_vector_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    263\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                         \u001b[0menv_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                     ) = self.envs[i].step(action)\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoreset_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mAutoresetMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# assumes that the user has correctly autoreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/utils/passive_env_checker.py\u001b[0m in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;34m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     assert isinstance(\n\u001b[1;32m    209\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             assert self.action_space.contains(\n\u001b[0m\u001b[1;32m    515\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             ), f\"{action!r} ({type(action)}) invalid \"\n",
            "\u001b[0;31mAssertionError\u001b[0m: tensor(1) (<class 'torch.Tensor'>) invalid "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we add to the epsilon-greedy strategy epsilon annealing. We change linearly epsilon from 1.0 to the value final_epsilon during first anneal_steps steps and then it remains on the final_epsilon level.\n",
        "Such an approach aims to increase the exploration level at the beginning of the training, when the Q-value estimate is poor and thus choosing greedily according to Q is not improving the performance."
      ],
      "metadata": {
        "id": "1iKU0YllgL5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EpsilonGreedyWithAnnealingDQN(EpsilonGreedyDQN):\n",
        "  def __init__(self, args):\n",
        "    self.start_epsilon = 1.0\n",
        "    super(EpsilonGreedyWithAnnealingDQN, self).__init__(args)\n",
        "    self.epsilon = self.start_epsilon\n",
        "    self.final_epsilon = 0.1 # investigate sensitivity\n",
        "    self.annealing = True\n",
        "    self.anneal_steps = 30000\n",
        "\n",
        "  def anneal(self, step):\n",
        "    # TODO\n",
        "    annealRate = (self.epsilon - self.final_epsilon) / self.anneal_steps\n",
        "    if(step <= self.anneal_steps):\n",
        "      self.epsilon -= annealRate\n",
        "    ####################################\n",
        "    ####################################\n",
        "\n",
        "  def reset(self):\n",
        "    super(EpsilonGreedyWithAnnealingDQN, self).reset()\n",
        "    self.epsilon = self.start_epsilon"
      ],
      "metadata": {
        "id": "4VRBq0D-_Ug7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_greedy_with_annealing_dqn = EpsilonGreedyWithAnnealingDQN(args)\n",
        "result_means_epsilon_greedy_with_annealing_dqn, result_stds_epsilon_greedy_with_annealing_dqn = train_dqn(epsilon_greedy_with_annealing_dqn)\n",
        "plot_results(result_means_epsilon_greedy_with_annealing_dqn, result_stds_epsilon_greedy_with_annealing_dqn)"
      ],
      "metadata": {
        "id": "ZY1C0OjtA8fV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9eda589d-c8bc-493b-ab4e-eb65ba9995f6"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step: 0\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 1000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 2000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 3000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 4000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 5000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 6000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 7000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 8000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 9000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 10000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 11000\n",
            "Eval mean: -91.60595854459508; eval_std: 8.340611914343942\n",
            "Training step: 12000\n",
            "Eval mean: -132.75707243792127; eval_std: 21.475015589912026\n",
            "Training step: 13000\n",
            "Eval mean: -143.20334330232762; eval_std: 19.332269877558733\n",
            "Training step: 14000\n",
            "Eval mean: -138.6191275919955; eval_std: 16.36727921131594\n",
            "Training step: 15000\n",
            "Eval mean: -144.57683662352534; eval_std: 11.083879884940337\n",
            "Training step: 16000\n",
            "Eval mean: -33.09826724564804; eval_std: 9.177491145982609\n",
            "Training step: 17000\n",
            "Eval mean: -44.345852981463615; eval_std: 8.292832490904475\n",
            "Training step: 18000\n",
            "Eval mean: -59.532316969978865; eval_std: 11.40720558982043\n",
            "Training step: 19000\n",
            "Eval mean: -59.857833620177566; eval_std: 5.220349845950594\n",
            "Training step: 20000\n",
            "Eval mean: -60.900164344830806; eval_std: 8.070623176885633\n",
            "Training step: 21000\n",
            "Eval mean: -74.63001881057377; eval_std: 10.487644766650625\n",
            "Training step: 22000\n",
            "Eval mean: -101.68845549575498; eval_std: 16.495807953868734\n",
            "Training step: 23000\n",
            "Eval mean: -83.60553698541146; eval_std: 3.980443922332225\n",
            "Training step: 24000\n",
            "Eval mean: -82.82628103553172; eval_std: 9.385634156935136\n",
            "Training step: 25000\n",
            "Eval mean: -82.31692101890293; eval_std: 7.016723987221535\n",
            "Training step: 26000\n",
            "Eval mean: -102.2996347782021; eval_std: 16.24091633661899\n",
            "Training step: 27000\n",
            "Eval mean: -63.35904355090756; eval_std: 4.366294232722529\n",
            "Training step: 28000\n",
            "Eval mean: -76.99479965368825; eval_std: 14.995879499711242\n",
            "Training step: 29000\n",
            "Eval mean: 27.271554664218133; eval_std: 12.006554620102035\n",
            "Training step: 30000\n",
            "Eval mean: 10.011631087125712; eval_std: 15.924409791227824\n",
            "Training step: 31000\n",
            "Eval mean: -131.22951828501948; eval_std: 29.730646459738765\n",
            "Training step: 32000\n",
            "Eval mean: 34.91806448800059; eval_std: 12.946438303365555\n",
            "Training step: 33000\n",
            "Eval mean: 29.065006301578524; eval_std: 15.604549374041682\n",
            "Training step: 34000\n",
            "Eval mean: 64.43130565021713; eval_std: 29.014174469352188\n",
            "Training step: 35000\n",
            "Eval mean: 173.26355967804977; eval_std: 39.85258992246632\n",
            "Training step: 36000\n",
            "Eval mean: 172.2812414102522; eval_std: 46.795371486070174\n",
            "Training step: 37000\n",
            "Eval mean: 192.193741593994; eval_std: 48.37898707663102\n",
            "Training step: 38000\n",
            "Eval mean: 162.59092526996506; eval_std: 52.923926864808834\n",
            "Training step: 39000\n",
            "Eval mean: 157.0482378668174; eval_std: 32.364170019761055\n",
            "Training step: 40000\n",
            "Eval mean: 161.34025038101018; eval_std: 32.288778129158125\n",
            "Training step: 41000\n",
            "Eval mean: 199.51050295929198; eval_std: 49.710315441747106\n",
            "Training step: 42000\n",
            "Eval mean: 195.2656218762743; eval_std: 38.858179866239084\n",
            "Training step: 43000\n",
            "Eval mean: 164.20951157144154; eval_std: 37.57757126851177\n",
            "Training step: 44000\n",
            "Eval mean: 167.49924395408013; eval_std: 72.4022480432306\n",
            "Training step: 45000\n",
            "Eval mean: 194.80293699339768; eval_std: 26.518410631957586\n",
            "Training step: 46000\n",
            "Eval mean: 207.51689465600145; eval_std: 36.27654666010804\n",
            "Training step: 47000\n",
            "Eval mean: 186.1439038714033; eval_std: 32.20235729066684\n",
            "Training step: 48000\n",
            "Eval mean: 127.09628765919945; eval_std: 113.25004049936459\n",
            "Training step: 49000\n",
            "Eval mean: 203.591966709276; eval_std: 58.75311738738523\n",
            "Training step: 50000\n",
            "Eval mean: 103.84670080062112; eval_std: 106.67661914004586\n",
            "Training step: 51000\n",
            "Eval mean: 105.61937679217755; eval_std: 80.7670402647952\n",
            "Training step: 52000\n",
            "Eval mean: 236.1248784101634; eval_std: 14.995618017975785\n",
            "Training step: 53000\n",
            "Eval mean: 241.20501389718348; eval_std: 8.48119098880319\n",
            "Training step: 54000\n",
            "Eval mean: 235.31095974162622; eval_std: 41.044386322175676\n",
            "Training step: 55000\n",
            "Eval mean: 167.14518748018324; eval_std: 25.82715724920429\n",
            "Training step: 56000\n",
            "Eval mean: 106.80012335809543; eval_std: 109.12111890434853\n",
            "Training step: 57000\n",
            "Eval mean: 60.0688583029471; eval_std: 59.57533294962207\n",
            "Training step: 58000\n",
            "Eval mean: 109.3746481337232; eval_std: 70.68753041667566\n",
            "Training step: 59000\n",
            "Eval mean: 108.73700230263442; eval_std: 108.98591317845242\n",
            "Training step: 60000\n",
            "Eval mean: -51.14068907970297; eval_std: 94.49256953925274\n",
            "Training step: 61000\n",
            "Eval mean: 129.80233725617992; eval_std: 52.67982178916517\n",
            "Training step: 62000\n",
            "Eval mean: 104.07389459791379; eval_std: 75.67920110778935\n",
            "Training step: 63000\n",
            "Eval mean: 195.23262093098242; eval_std: 52.77145813460836\n",
            "Training step: 64000\n",
            "Eval mean: 153.6464569238719; eval_std: 43.82187901705065\n",
            "Training step: 65000\n",
            "Eval mean: -3.4867501292436223; eval_std: 115.39140929759337\n",
            "Training step: 66000\n",
            "Eval mean: 95.59950247233827; eval_std: 74.8846031196295\n",
            "Training step: 67000\n",
            "Eval mean: 140.7969184218317; eval_std: 63.545949738029435\n",
            "Training step: 68000\n",
            "Eval mean: 167.00905550483907; eval_std: 49.074475726771496\n",
            "Training step: 69000\n",
            "Eval mean: 207.85467173915038; eval_std: 56.440342031118135\n",
            "Training step: 70000\n",
            "Eval mean: 226.87490805853804; eval_std: 9.949584423863243\n",
            "Training step: 71000\n",
            "Eval mean: 166.37792948055395; eval_std: 58.06388617357977\n",
            "Training step: 72000\n",
            "Eval mean: 181.36981299110587; eval_std: 47.35695987710094\n",
            "Training step: 73000\n",
            "Eval mean: 233.0127270034951; eval_std: 16.261166063011405\n",
            "Training step: 74000\n",
            "Eval mean: 163.2172952295322; eval_std: 37.02120776333821\n",
            "Training step: 75000\n",
            "Eval mean: 221.94393908215298; eval_std: 30.597665737430614\n",
            "Training step: 76000\n",
            "Eval mean: 200.2362138454363; eval_std: 30.87007643349934\n",
            "Training step: 77000\n",
            "Eval mean: 140.882158362795; eval_std: 74.41342317529386\n",
            "Training step: 78000\n",
            "Eval mean: 162.45343428706562; eval_std: 69.69353447887565\n",
            "Training step: 79000\n",
            "Eval mean: 134.5836765359883; eval_std: 48.2074444453331\n",
            "Training step: 80000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-763915477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepsilon_greedy_with_annealing_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEpsilonGreedyWithAnnealingDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_means_epsilon_greedy_with_annealing_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_epsilon_greedy_with_annealing_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon_greedy_with_annealing_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_means_epsilon_greedy_with_annealing_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_epsilon_greedy_with_annealing_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-503547981.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(dqn)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186968863.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training step: {step}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m           \u001b[0meval_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Eval mean: {eval_mean}; eval_std: {eval_std}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m           \u001b[0meval_results_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186968863.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m           \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m           \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m           \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/vector/sync_vector_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    263\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                         \u001b[0menv_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                     ) = self.envs[i].step(action)\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoreset_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mAutoresetMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0;31m# assumes that the user has correctly autoreset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \"\"\"\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    325\u001b[0m     ) -> tuple[WrapperObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n\u001b[1;32m    326\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     def reset(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    665\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternative approach to the epsilon-greedy strategy is to use so-called boltzmann exploration strategy.\n",
        "The idea behind this approach is to perform softmax on the Q-values coming from the main critic and then sample from the obtained distribution.\n",
        "In this approach we use softmax with a temperature, i.e. before applying softmax, we scale all the Q-values by the temperature coefficient (in the literature we usually divide by the temperature, but this is equivallent to scaling by the inverse of the temperature). Large scaling values make the distribution close to the greedy choice, while low scaling values make the distribution close to the uniform one."
      ],
      "metadata": {
        "id": "2VJ4wifPiE55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BoltzmannDQN(DQN):\n",
        "  def __init__(self, args):\n",
        "    super(BoltzmannDQN, self).__init__(args)\n",
        "    self.temperature = 1# investigate sensitivity\n",
        "\n",
        "  def explore(self, states):\n",
        "    action = None\n",
        "    with torch.no_grad():\n",
        "      # TODO\n",
        "      qValues = self.q_net(states) # this should give us raw qValues\n",
        "\n",
        "      qValues /= self.temperature\n",
        "\n",
        "      sm = torch.nn.Softmax(dim=-1)\n",
        "      qValues = sm(qValues)\n",
        "\n",
        "      action = torch.stack([torch.multinomial(qValues[dim0, dim1],1) for dim0 in range(qValues.shape[0]) for dim1 in range(qValues.shape[1])])\n",
        "      action = action.squeeze(-1)\n",
        "      action = torch.Tensor.tolist(action)\n",
        "\n",
        "    return action"
      ],
      "metadata": {
        "id": "_0wB5TcYRGAp"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boltzmann_dqn = BoltzmannDQN(args)\n",
        "result_means_boltzmann_dqn, result_stds_boltzmann_dqn = train_dqn(boltzmann_dqn)\n",
        "plot_results(result_means_boltzmann_dqn, result_stds_boltzmann_dqn)"
      ],
      "metadata": {
        "id": "0wcy8QOyijxg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "b41633f9-f665-47c5-a392-a2d6fd875564"
      },
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step: 0\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 1000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 2000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 3000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n",
            "Training step: 4000\n",
            "Eval mean: -639.6903477514232; eval_std: 59.38467170955477\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2425147962.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mboltzmann_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoltzmannDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult_means_boltzmann_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_boltzmann_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboltzmann_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplot_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_means_boltzmann_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_stds_boltzmann_dqn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-503547981.py\u001b[0m in \u001b[0;36mtrain_dqn\u001b[0;34m(dqn)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4186968863.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_training_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannealing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manneal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1047931589.py\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mqValues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqValues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqValues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim0\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdim1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the compromises between epsilon-greedy and boltzmann exploration strategy is so-calles max-boltzmann strategy. In this strategy with probability 1-epsilon we choose action greedily, but with probability epsilon we perform the boltzmann choice instead of the uniform random choice."
      ],
      "metadata": {
        "id": "oLyezLVijx3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxBoltzmannDQN(EpsilonGreedyWithAnnealingDQN):\n",
        "  def __init__(self, args):\n",
        "    super(MaxBoltzmannDQN, self).__init__(args)\n",
        "    self.temperature = 0.1 # investigate sensitivity\n",
        "\n",
        "  def explore(self, states):\n",
        "    action = None\n",
        "    with torch.no_grad():\n",
        "      # TODO\n",
        "      ####################################\n",
        "      ####################################\n",
        "    return action"
      ],
      "metadata": {
        "id": "5uaDREL-hIkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_boltzmann_dqn = MaxBoltzmannDQN(args)\n",
        "result_means_max_boltzmann_dqn, result_stds_max_boltzmann_dqn = train_dqn(max_boltzmann_dqn)\n",
        "plot_results(result_means_max_boltzmann_dqn, result_stds_max_boltzmann_dqn)"
      ],
      "metadata": {
        "id": "QjR01lGcw1jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to adjusting the value of epsilon in epsilon-greedy strategy, we can adjust the temperature in the max-boltzmann and boltzmann strategies: we start we the value start_temperature and linearly increase the value to the final_temperature during temperature_anneal_steps, then the temperature is on the constant level.\n"
      ],
      "metadata": {
        "id": "rq_pxa8KkbIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxBoltzmannWithTemperatureAnnealingDQN(MaxBoltzmannDQN):\n",
        "  def __init__(self, args):\n",
        "    self.start_temparature = 0.025 # investigate sensitivity\n",
        "    super(MaxBoltzmannWithTemperatureAnnealingDQN, self).__init__(args)\n",
        "    self.temperature = self.start_temparature\n",
        "    self.final_temperature = 0.3 # investigate sensitivity\n",
        "    self.temperature_anneal_steps = 30000\n",
        "    self.annealing = True\n",
        "\n",
        "  def anneal(self, step):\n",
        "    super(MaxBoltzmannWithTemperatureAnnealingDQN, self).anneal(step)\n",
        "    # TODO\n",
        "    ####################################\n",
        "    ####################################\n",
        "\n",
        "  def reset(self):\n",
        "    super(MaxBoltzmannWithTemperatureAnnealingDQN, self).reset()\n",
        "    self.temperature = self.start_temparature"
      ],
      "metadata": {
        "id": "3naKqfLVDwP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_boltzmann_temp_anneal_dqn = MaxBoltzmannWithTemperatureAnnealingDQN(args)\n",
        "result_means_max_boltzmann_temp_anneal_dqn, result_stds_max_boltzmann_temp_anneal_dqn = train_dqn(max_boltzmann_temp_anneal_dqn)\n",
        "plot_results(result_means_max_boltzmann_temp_anneal_dqn, result_stds_max_boltzmann_temp_anneal_dqn)"
      ],
      "metadata": {
        "id": "HidAoBNCFV6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BoltzmannWithTemperatureAnnealingDQN(BoltzmannDQN):\n",
        "  def __init__(self, args):\n",
        "    self.start_temparature = 0.25 # investigate sensitivity\n",
        "    super(BoltzmannWithTemperatureAnnealingDQN, self).__init__(args)\n",
        "    self.temperature = self.start_temparature\n",
        "    self.final_temperature = 3.0 # investigate sensitivity\n",
        "    self.temperature_anneal_steps = 30000\n",
        "    self.annealing = True\n",
        "\n",
        "  def anneal(self, step):\n",
        "    # TODO\n",
        "    ####################################\n",
        "    ####################################\n",
        "\n",
        "  def reset(self):\n",
        "    super(BoltzmannWithTemperatureAnnealingDQN, self).reset()\n",
        "    self.temperature = self.start_temparature"
      ],
      "metadata": {
        "id": "ylINCOiLb09P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boltzmann_temp_anneal_dqn = BoltzmannWithTemperatureAnnealingDQN(args)\n",
        "result_means_boltzmann_temp_anneal_dqn, result_stds_boltzmann_temp_anneal_dqn = train_dqn(boltzmann_temp_anneal_dqn)\n",
        "plot_results(result_means_boltzmann_temp_anneal_dqn, result_stds_boltzmann_temp_anneal_dqn)"
      ],
      "metadata": {
        "id": "R--aoCSzjEjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last exploration idea we want to implement is a combintation of the epsilon-greedy strategy (with epsilon annealing) and the boltzmann strategy.\n",
        "We could think that at the beginning of the training the boltzmann strategy struggles because the Q-function (the main critic) is not yet well-trained. However, the more critic is trained, the more sense it makes to start using the boltzmann strategy. We would like to verif y this hypoothesis by using in the first half of the training epsilon-greedy strategy (with epsilon annealing) and in the second half of the training switch the exploration strategy to the boltzmann one."
      ],
      "metadata": {
        "id": "-jcSRBHXldKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_two_halfs_dqn(dqn_1, dqn_2):\n",
        "  set_seed(TRAIN_SEED)\n",
        "  # TODO\n",
        "  ####################################\n",
        "  ####################################\n",
        "\n",
        "  return result_mean, result_std\n"
      ],
      "metadata": {
        "id": "AjaSq-vC1PTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epsilon_greedy_with_annealing_half_dqn = EpsilonGreedyWithAnnealingDQN(first_half_training_args) # investigate sensitivity of epsilon\n",
        "epsilon_greedy_boltzmann_half_dqn = BoltzmannDQN(second_half_training_args) # investigate sensitivity of temperature\n",
        "result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn, result_stds_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn = train_two_halfs_dqn(epsilon_greedy_with_annealing_half_dqn, epsilon_greedy_boltzmann_half_dqn)\n",
        "plot_results(result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn, result_stds_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn)"
      ],
      "metadata": {
        "id": "YwVivxWM1pXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we plot the results of all exploration methods on one plot. However, for drawing conclusions, it might be reasonable to plot some subsets of methods together, for example to compare variants with and without annealing, max-boltzmann with boltzmann, epsilon-greedy, boltzmann and half-epsilon-greedy, half-boltzmann."
      ],
      "metadata": {
        "id": "EM7aIGQuQ8Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_means_list = [result_means_dqn, result_means_epsilon_greedy_dqn, result_means_epsilon_greedy_with_annealing_dqn,\n",
        "                result_means_boltzmann_dqn, result_means_boltzmann_temp_anneal_dqn,\n",
        "                result_means_max_boltzmann_dqn, result_means_max_boltzmann_temp_anneal_dqn,\n",
        "                result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn]\n",
        "result_stds_list = [result_stds_dqn, result_stds_epsilon_greedy_dqn, result_stds_epsilon_greedy_with_annealing_dqn,\n",
        "                result_stds_boltzmann_dqn, result_stds_boltzmann_temp_anneal_dqn,\n",
        "                result_stds_max_boltzmann_dqn, result_stds_max_boltzmann_temp_anneal_dqn,\n",
        "                result_stds_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn]\n",
        "names_list = [\"random\", \"epsilon-greedy\", \"epsilon-greedy-with-annealing\",\n",
        "             \"boltzmann\", \"boltzmann-with-annealing\",\n",
        "             \"max-boltzmann\", \"max-boltzmann-with-annealing\",\n",
        "             \"half-epsilon-greedy-with-annealing_half-boltzmann\"]\n",
        "colours_list = [\"red\", \"green\", \"blue\",\n",
        "           \"yellow\", \"magenta\",\n",
        "           \"cyan\", \"black\",\n",
        "           \"orange\"]\n",
        "\n",
        "plot_results_many(result_means_list, result_stds_list, names_list, colours_list)"
      ],
      "metadata": {
        "id": "yDgWDm0IM4OS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "9f198fc9-a931-40e3-ac1f-bed30076426d"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'result_means_epsilon_greedy_with_annealing_dqn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-832443868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result_means_list = [result_means_dqn, result_means_epsilon_greedy_dqn, result_means_epsilon_greedy_with_annealing_dqn,\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mresult_means_boltzmann_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_means_boltzmann_temp_anneal_dqn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mresult_means_max_boltzmann_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_means_max_boltzmann_temp_anneal_dqn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 result_means_epsilon_greedy_with_annealing_half_epsilon_greedy_boltzmann_half_dqn]\n\u001b[1;32m      5\u001b[0m result_stds_list = [result_stds_dqn, result_stds_epsilon_greedy_dqn, result_stds_epsilon_greedy_with_annealing_dqn,\n",
            "\u001b[0;31mNameError\u001b[0m: name 'result_means_epsilon_greedy_with_annealing_dqn' is not defined"
          ]
        }
      ]
    }
  ]
}